
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>BI blog 4U</title>
  <meta name="author" content="Souichi Narumiya">

  
  <meta name="description" content="原文 A Brief Introduction to Statistics – Part 1
Posted on October 28, 2014 | Leave a comment
What is Statistics?
Collected observations are called &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://narusou.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="BI blog 4U" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet" type="text/css">
<!--- MathJax Configuration -->
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <meta property="og:title" content="BI blog 4U" />
<meta property="og:description" content="原文 A Brief Introduction to Statistics – Part 1
Posted on October 28, 2014 | Leave a comment
What is Statistics?
Collected observations are called &hellip;" />
<meta property="og:url" content="http://narusou.github.io/" />
<meta property="og:image" content="http://narusou.github.io" />
<meta property="og:author" content="Souichi Narumiya" />
<meta property="og:site_name" content="BI blog 4U" />
<meta property="og:locale" content="ja_JP" />
<meta property="og:type" content="article" />
<meta property="fb:app_id" content="1735411246487943" />


</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">BI blog 4U</a></h1>
  
    <h2>articles and translations</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscribe" data-subscription="rss email">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 13.310204,73.332654 C 5.967347,73.332654 0,79.322448 0,86.621428 c 0,7.338776 5.967347,13.262246 13.310204,13.262246 7.370408,0 13.328572,-5.92245 13.328572,-13.262246 0,-7.29898 -5.958164,-13.288774 -13.328572,-13.288774 z M 0.01530612,33.978572 V 53.143878 C 12.493878,53.143878 24.229592,58.02347 33.068368,66.865306 41.894898,75.685714 46.767346,87.47449 46.767346,100 h 19.25 C 66.017346,63.592858 36.4,33.979592 0.01530612,33.978572 l 0,0 z M 0.03877552,0 V 19.17449 C 44.54796,19.17551 80.77551,55.437756 80.77551,100 H 100 C 100,44.87653 55.15102,0 0.03877552,0 z"></path></svg></a></li>
  
    <li><a href="narumiyasouichi@yahoo.co.jp" rel="subscribe-email" title="subscribe via email" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" height="25" viewbox="0 0 100 100"><path class="social" d=""m 154.85133,65.376619 -18.75652,19.018743 60.02087,0 -18.75653,-19.018743 -11.25391,14.264058 z m 48.76696,-33.282799 -22.50783,28.528113 22.50782,23.773429 z m -75.02609,0 0,52.301542 c 0,0 20.70458,-21.967007 22.50783,-23.773429 z m 7.50261,-2e-6 30.01043,38.037487 30.01044,-38.037487 z""></path></svg></a></li>
  
</ul>
  
  
  
  
  
  
  
  
  
  
    
      <form action="https://www.google.com/search" method="get">
        <fieldset role="search">
          <input type="hidden" name="sitesearch" value="narusou.github.io" />
    
          <input class="search" type="text" name="q" results="0" placeholder="Search"/>
        </fieldset>
      </form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/24/a-brief-intoroduction-to-statistics-part1/">A Brief Intoroduction to Statistics - Part1</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-24T13:46:42+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>24</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>1:46 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><h3>原文</h3>

<p>A Brief Introduction to Statistics – Part 1
Posted on October 28, 2014 | Leave a comment
What is Statistics?
Collected observations are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data. Each observation in data is called a case. Characteristics of the case are called variables. With a matrix/table analogy, a case is a row while a variable is a column.
Statistics - Correlation
Statistics – Correlation (Courtesy: xkcd.com)
Types of variables:
Numerical– Can be discrete or continuous, and can take a wide range of numerical values.
Categorical– Specific or limited range of values, usually called levels. Variables with natural ordering of levels are called ordinal categorical variables.
A pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent.
Data collected in haphazard fashion are called anecdotal evidence. Such evidence may be true and veriﬁable, but it may only represent extraordinary cases.
There are two main types of scientific data collection:
Observational studies – collection of data without interfering with how the data has arisen. Can provide evidence of a naturally occurring association between variables, but by themselves, cannot show a causal connection.
Experiments – randomized experiments, usually with an explanatory variableand a response variable are performed, often with a control group.
In general, correlation does not imply causation, and causation can only be inferred from a randomized experiment.
Types of sampling:
Simple random sampling: Each subject in the population is equally likely to be selected.
Stratified sampling: The population is first divided into homogeneous strata (subjects within each stratum are similar, but different across strata) followed by random sampling from within each stratum.
Cluster sampling: The population is first divided into groups or clusters (subjects within each cluster are non-homogeneous, but clusters are similar to each other). Next a few clusters are randomly sampled followed by random sampling from within each cluster.
Randomized experiments are generally built on four principles:
Controlling – control any differences between groups for confounding variables which are known and can be accounted for.
Randomization – randomize population into groups to account for variables that cannot be controlled.
Replication – collect sufficiently large sample or replicate entire study to improve estimation.
Blocking – advanced technique of grouping population based on variable known/suspected to influence response, followed by randomizing cases within the group.
Reducing bias in experiments –
Randomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and eﬀect relationships in all cases. Blinding can help in overcoming placebo effect in human studies.
Distributions of a numerical variable are described by shape, center and spread. The three most commonly used measures of center and spread are:
center: mean (the arithmetic average), median (the midpoint), mode (the most frequent observation)
spread: standard deviation (variability around the mean), range (max-min), interquartile range IQR (middle 50% of the distribution)
An outlier is an observation that appears extreme relative to the rest of the data.
A robust statistic (e.g. median, IQR) is a statistic that is not heavily affected by skewness and extreme outliers.
Comparing categorical data:
A table that summarizes data for two categorical variables in this way is called a contingency table. A table for a single variable is called a frequency table. A bar plot is a common way to display a single categorical variable. A segmented bar plot is a graphical display of contingency table information. A mosaic plot is a graphical display of contingency table information that is similar to a bar plot for one variable or a segmented bar plot when using two variables. While pie charts are well known, they are not typically as useful as other charts in a data analysis.
Comparing numerical data:
The side-by-side box plot is a traditional tool for comparing across groups. Another useful plotting method uses hollow histograms to compare numerical data across groups.
Hypothesis test:
H0 Independence model – Explanatory variable has no eﬀect on response variable, and we observed a diﬀerence that would only happen rarely.
HA Alternative model – Explanatory variable has an eﬀect on response variable, and what we observed was actually due to explanatory variable effect on the response variable explaining the difference.
Based on the simulations, we have two options:
1. We conclude that the study results do not provide strong evidence against the independence model.
2. We conclude the evidence is suﬃciently strong to reject H0 and assert the alternative hypothesis.
When we conduct formal studies, usually we reject the notion that we just happened to observe a rare event. So in such a case, we reject the independence model in favor of the alternative.
Statistical Inference:
One ﬁeld of statistics, statistical inference, is built on evaluating whether such differences are due to chance. In statistical inference, statisticians evaluate which model is most reasonable given the data. Errors do occur, just like rare events, and we might choose the wrong model. While we do not always choose correctly, statistical inference gives us tools to control and evaluate how often these errors occur.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/24/designing-the-future-data-innovation-labs/">Designing the Future -Data Innovation LAbs</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-24T13:44:46+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>24</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>1:44 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><h3>原文</h3>

<p>Designing the future – Data Innovation Labs
Posted on January 19, 2015 | Leave a comment
With the ongoing Big data revolution, and the impending Internet of Things revolution, there has been a renewed enthusiasm in “innovation” around data.  Similar to the Labs concept started by Google (think Gmail Beta based on Ajax, circa 2004), more and more organizations, business communities, governments and countries are setting up Labs to foster innovation in data and analytics technologies. The idea behind these “data innovation labs” is to develop avant-garde data and analytics technologies and products in an agile fashion and move quickly from concept to production. Given the traditional bureaucratic setup in large organizations and governments, these Labs stand a better chance of fostering a culture of innovation, due to their being autonomous entities and their startup-mode culture leveraging agile methodologies.
Data Innovation Labs
Data Innovation Labs
Below I list a few of the data innovation labs that have been setup to get value for their parent entities in the data and analytics space, trying to build data products in the Big Data and Data Science fields.</p>

<pre><code>Data Innovation Lab – Thomson Reuters
    Small group of about ten people, partnering with internal teams, third-parties and customers to find data-driven innovations
    experiments with mash-ups of internal and external data in novel ways
    hosts internal crowd-sourcing competitions
    translate business problem into technical data problem statement
    created Exchange – a digital forum for sharing ideas and insights
    Partners with Central Strategy to estimate potential and market-size for new data innovation opportunities
The Data Lab – Scotland
    mission is to strengthen Scotland’s local industry and transfer world-leading research in informatics and computer science in the global marketplace
    focuses on skills and training by working with industry  to create  a pipeline of talented data scientists equipped with the relevant skills
    connects world-leading researchers and data scientists with local industry and public sector organizations, giving them access to experts who can help collaborate on solutions to key problems
Smart Data Innovation Lab – German government
    Hosted at Karlsruhe Institute for Technology, its mission is to turn big data into smart data
    plans to store data centrally in a highly secured environment for research purposes
    has cutting-edge insfrastructure for processing Big Data including software like SAP HANA, IBM Watson and hardware on IBM Power and Intel architectures
    Industry partners to deliver data sources directly from the practice environment, to be complemented with crowdsourced data and open data
    plans to offer an open source repository for reuse in research
Midata Innovation Lab – UK government
    An organization run by the Department of  Business Innovation &amp; Skills with involvement of industry
    Accelerator for businesses to create new services for consumers, from data
    work involves concept of personal data stores (PDS) or personal clouds
    working with three leading PDS – Allfiled, Mydex and Paoga
    Participating organizations and developers use PDS to create new innovative services for consumers
Nordstrom Innovation Lab – Nordstrom
    Internal technology lab focused on innvoation around technology
    Secondary focus – but still in scope: operations, products, business models and management
    Goal is to deliver data-driven products to inform business decisions internally, and to enhance customer experience externally
    Multi-disciplinary team of techies, designers, entrepreneurs, statisticians, researchers and artists
GFDRR Innovation Lab – World Bank
    Global facility for disaster reduction and recovery, a global partnership, managed by the World Bank and funded by 25 donor partners
    supports use of science, technology, open data and innovation to empower decision-makers to increase their resilience
    tries to apply the concepts of the global open data movement to the challenges of reducing vulnerability to natural hazards and  the impacts of climate change through OpenDRI (Open Data for Resilience Initiative)
Big Data Innovation Center and Innovation Lab – SAP
    Focus on SAP’s mobile and cloud portfolio
    Mission is to extend SAP stack and develop innovative data-driven process applications leveraging an integrated platform and next-generation DB technologies
    Partnership and exchanges with leading schools including Stanford, MIT, Berlin universities
    Short, fast-paced innovation cycles
    Project run-times of a few months on an average
    Hands over prototypes to SAP development for turning into market-ready products
</code></pre>

<h3>翻訳</h3>

<p>データイノベーションのラボ、未来をデザインする。</p>

<p>ビッグデータ革命の真っ最中、そして差し迫ったIOT革命、データを中心にしたイノベーションの中にあらたな熱狂がそこにはある。
Googleによってはじめられた複数のラボのコンセプトのようなラボが、より多くの組織、ビジネスコミュニティ、政府と国によって建てられ、データと分析の技術のイノベーションを育てるためにラボをスタートアップしている。
これらのデータイノベーションラボの背景にある考えは、アヴァンギャルドデータ、分析技術、Agile fashionの中での商品とコンセプトから製品化までの速い動きを開発するという点にある。
巨大な組織や政府内部で伝統的な官僚性の設定の元、これらのラボはイノベーションの文化を育てるよりよいチャンスをつくり、彼らの自主的な独立存在のために彼らのスタートアップの流儀による文化はアジャイルメソドロジーに影響を与えている。</p>

<p>下記に、データと分析の分野にある親組織のために価値を得るために設置された、いくつかのデータイノベーションラボをリストしている。ビッグデータとデータサイエンスの分野でデータ商品を作成を試みて作成されている。</p>

<pre><code>Data Innovation Lab – Thomson Reuters
    １０人ほどの少ないグループ、内部のチーム、サードパーティー、顧客とデータ運用のイノベーションを見つけるためにパートナとなっている。
    刷新された方法での内部データと外部データのマッシュアップによる実験
    内部のクラウドソースコンペティションをホストしている。
    ビジネスプロブレムを技術データ問題の文章に翻訳している。
    Exchangeを作成：考えや洞察をきょうゆうするためのデジタルフォーラム
    新しいデータイノベーションの機会のためにマーケットサイズと潜在性の見積もりをするための中央戦略とパートナ－である。
The Data Lab – Scotland
    ミッションはスコットランドの地元産業を強くすることである。また、世界市場の情報科学とコンピュータサイエンスにおいて世界をリードする研究へと移行することだ。
    適切なスキルで武装された才能あふれるデータサイエンティストたちのパイプラインを作成するために産業と働くことによりトレーニングとスキルに焦点をあてている。
    世界を牽引する研究者たちと地元の産業と公共部門の組織を通し、データサイエンティストと結び付け、主要な問題を解決するための協力を要請できるようなエキスパートにアクセスさせている。
Smart Data Innovation Lab – German government
    カールスルーエ工科大学（KIT）によってホストされている。そのミッションはビッグデータをスマートデータに落とし込むことだ。
    研究目的のために非常にセキュアな環境に、データを中央に貯蔵する計画をしている。
    SAP HANAやIBM Watoson, ハードウェアはIBM Power, Intel architecturesなど、ビッグデータをプロセスするための最新のインフラを揃えている。
    クラウドソース、オープンソースのデータで補完された実行環境から直接、データソースを出力する産業パートナー
    調査内で再利用するためにオープンソースレポジトリを提供する計画を立てている。
Midata Innovation Lab – UK government
    UKのBIS(ビジネスイノベーションと産業関連能力部)によって経営されている組織
    ビッグデータから、消費者のための新しいサービスを作成するビジネスを後押しする。
    個人クラウド、もしくは個人データストアのコンセプトを含み、動いている。
    PaogaとMydex、Allfieldという三つのＰＤＳのリードで働いている。
    参加している組織や開発者はPDSを使用して、新しい革新的なサービスを消費者に提供します。
Nordstrom Innovation Lab – Nordstrom
    内部の技術ラボは技術の周辺のイノベーションに焦点を当てている。
    第二の焦点ーしかしスコープに入っている点：運用、製品化、ビジネスモデルとマネジメント
    ゴールは内的にビジネスの決定を行うためのデータ運用製品をデリバリすること
    技術者と、デザイナー、起業家、統計学者、研究者とアーティストなどの複数分野にわたるチーム
GFDRR Innovation Lab – World Bank
    災害削減と復旧のための国際組織で、25の寄付パートナーにファンドされ世界銀行によってマネージされているグローバルパートナーシップ。
    科学と技術、オープンデータとイノベーションを利用し、ディシジョンメーカーが彼らの復興を増加させるために力添えしている。
    OpenDRI (Open Data for Resilience Initiative)を通して自然災害と気候変動のインパクトへの脆弱性を減じる挑戦へと世界のオープンデータの動きのコンセプトを適用させようとしている。
Big Data Innovation Center and Innovation Lab – SAP
    SAPのモバイルとクラウドのポートフォリオにフォーカス
    ミッションはSAPのスタックを拡張することと、次の世代のDB技術と統合プラットフォームをテコ入れするイノベイティブなデータ運用プロセスアプリの開発。
    スタンフォード、ベルリン、ミットを含んだ牽引学校とパートナー、交換を行っている。
    短く、速いイノベーションサイクル
    平均で数か月経営のプロジェクト
    市場に出す準備の製品のプロトタイプをSAPの開発者に引き渡している。
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/24/a-gentle-introduction-to-machine-learning/">A Gentle Introduction to Machine Learning</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-24T13:42:30+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>24</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>1:42 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><h3>原文</h3>

<p>A gentle introduction to Machine Learning
Posted on February 5, 2015 | Leave a comment
Machine Learning is a big part of big data and data science. A subset of artificial intelligence – a branch of science notorious for requiring advanced knowledge of mathematics. In practice though, most data scientists don’t try to build a Chappie  and there are simpler, practical ways to get started with machine learning.
Gmail Priority Inbox
Gmail Priority Inbox
Machine learning in practice involves predictions based on data. Notable examples include Amazon’s product recommendations with the “customers also bought” scroll-list, or Gmail’s priority inbox or any email spam-filter feature. How do these work? For Amazon, clicks by the user is used to learn and predict user behavior and propensity (likelihood) to buy certain items. The items the user is most likely to buy are then displayed on the recommendation system. Gmail’s system learns from the messages which the user reads and/replies to and prioritizes them.
Amazon Recommendations
Amazon Recommendations
In both cases, there are some predictions made based on certain example usage of the data. Thus in essence, machine learning is about predictions based on models, which themselves are created based on examples.
More specifically, a machine learning model is a set of explanations of the relationships between the input data and the output predictions. These relationships are discovered from examples of input-output pairs. In machine learning terminology, the input data is also called features and the predictions are called output. Once a model has been created, it can be used on new inputs to predict outputs.
Machine learning models therefore “learn” to predict from examples. This learning is also known as “training” the model, and the associated good quality data-set is called “training data-set“. The stage where the model is used on new inputs is known as “testing” the model, and the data-set associated with it is called “test data-set“.
There are different ways to perform this learning, with different types of algorithms to build models and perform predictions. Most common among these are Classification and Regression techniques. The Gmail spam-filter is an example of Classification technique. Given a set of emails marked as spam or not-spam, it learns the characteristics of emails and is then able to process future email messages to mark them as spam or not. Classification deals with prediction of which class ordinal data fits in, while regression deals with prediction of continuous numeric data. Example of regression is a best-fit line drawn through some data points for generalization. Both classification and regression are examples of supervised learning, as the algorithm is told to predict a label or target value.
The opposite of this is unsupervised learning – where there’s no label or target value given for the data. An example of this could be clustering – a task of grouping a items so that objects in the same group or cluster are more similar (in some sense or another) to each other than to those in other groups (clusters).
Machine Learning Techniques
Machine Learning Techniques
With so many choices, how do you choose the right algorithm? Without considering nuances of the data, a rule of thumb is to look at the objective of the prediction:</p>

<pre><code>If the prediction is to forecast a target value, we use supervised learning, else use unsupervised learning or density estimation algorithms.
</code></pre>

<p>It is important to note though, that this is not unbreakable, rather usage of algorithms is rather fuzzy. This is quite common in machine learning, where most problems are not deterministic in nature, and often a bunch of different algorithms are tried out to see how they perform. There are also ensemble models like Gradient Boosting – a regression technique which  uses an ensemble of weak prediction models, typically decision trees to get an improved prediction model. An interesting tool based on symbolic regression, which infers the model from the data is Nutonian Eureqa, also dubbed as the robotic data scientist.
Many algorithms are different, but the steps to use one are similar:
Collect data > Prepare data > Analyze input data > Clean data/verify data quality > Train the algorithm > Test the algorithm > Iterate/Deploy. (See also my earlier post on the data science project lifecycle)
As with many other aspects (data wrangling) of data science, both R and Python are very popular languages for using machine learning techniques. There are also start-ups like BigML providing MLaaS or Machine Learning as a Service.
In conclusion of this post, a few points to remember: garbage in – garbage out:- data quality matters as much if not more than algorithms, quantity of data or complexity of algorithm are not substitutes for quality, and of course as with all predictions, machine learning can be wrong as well.</p>

<h3>翻訳</h3>

<p>機械学習への優しい導入</p>

<p>ビッグデータの大部分である機械学習といえる。人工知能の一部で、高度の数学知識を求める悪名高い科学の部門として知られている。実践では、ほとんどのデータサイエンティストはChappieを作成しようとはせず、機械学習を始めるのにより簡単で実践的な方法がある。
機械学習は実践としてはデータをベースにした予測を伴っている。アマゾンのcustomer also boughtの商品推薦スクロール、Gmailの優先順位、スパムフィルターの特性を持つEmail等が目立った例として挙げられる。これらはどうやって動いているのか。
アマゾンの場合にはユーザーによるクリックによって学ばれ、そのユーザーの動きや、あるアイテムを買う傾向を予知している。このユーザーに買われそうなアイテムは、そうして推薦システム上にディスプレイされる。
Gmailのシステムはユーザーが読んだ、返事したメッセージから情報を学び、それらを優先順位付けする。</p>

<p>両方の場合でも、データの例示された確かな使用を基礎に予見がなされている。それゆえ主要な点として、予見に関する機械学習は例を基礎として作成されたモデルを基にしている。
さらに具体 的には、機械学習モデルはインプットされたデータとアウトプットされた予見の間の関係に対する纏まった説明であるともいえる。</p>

<p>これらはインプットとアウトプットのペアの例から発見されている。機械学習の技術では、インプットはfeatures[特徴]とよばれ、アウトプットはpredictions[予想]と呼ばれている。一度モデルが作られると、新しいインプットからアウトプットを予想するために使用される。
機械学習モデルは例から予想するために「学習」する。この学習はまた、モデルの「訓練」として知られており、この関連したよい品質のデータセットは「トレーニングデータセット」として呼ばれている。
新しいインプットの上でモデルが使用される段階はモデルを「テストする」としてしられており、これらに関連するデータセットは「テストデータセット」として知られている。</p>

<p>ここにはこの学習を実行するための異なる方法がいくつかある。これらの中でもっともよく知られているのは、「分類」と「回帰」の技術だ。Gmail Spam Filterは「分類」の技術の一例になる。スパムかそうでないのかといったマークがつけられたEmailのセットが与えられると、Emailの特徴を学び、そうすると未来のemailメッセージをスパムかそうでないかをマークするプロセスが可能になる。「分類」はどの分類普遍データがふさわしいかの予想を取り扱い、「回帰」は継続した数的なデータの予想を取り扱う。回帰の例は一般化のためのいくつかのデータポイントを通して導き出された最も適切な水準だ。分類も回帰も教師信号ありの学習の例で、目的の価値、もしくはラベルを予想するためのアルゴリズムとして示されている。</p>

<p>この反対が、教師信号なしの学習であり、そのデータのために与えられるような目的の値やラベルといったものはない。この例としてはクラスタリングが相当しうる。クラスタリングは項目をグルーピングするタスクで、同じグループ、もしくは他のクラスタの中の対象物はお互い、他のクラスタの内部の対象よりも（どこか、もしくは複数個所が）似ている。</p>

<p>なぜこんなにも複数の選択があるのか？どうやってそのアルゴリズムを学ぶのか？データのニュアンスを考慮せずに、予想の目的に注目することが親指ルール（大まかなやり方）だ。</p>

<p>・もしも対象の値を予想することが今回のpredictionであった場合には、私たちは教師あり信号を使用する。そうでなければ教師なし信号、密度見積もりアルゴリズムを使用する。</p>

<p>これに気づくのは重要だが、それは絶対のルールではなく、むしろアルゴリズムの使用方法はかなりファジーだ。これは機械学習ではかなり普通のことで、自然界ではほとんどの問題は決定論的ではなく、それらがどういうものかを確認するために多くの違ったアルゴリズムが試される。
またGradient Boostingのようなアンサンブルモデルもあり、弱い予見モデルのアンサンブルを使用した回帰技術を使用している。典型では、改善された予想モデルを取り入れるための意思決定ツリーのようなものがある。
記号回帰を基礎としたデータからモデルを推量するNutonian Euraqaという興味深いツールも存在している。これはロボティックデータサイエンティストとも呼ばれている。</p>

<p>多くのアルゴリズムがお互い異なり、一つのものを使うステップは似ている。</p>

<p>１、データ集め
２、データを準備
３、入力データを分析
４、データをクリア/データの品質を証明
５、アルゴリズムを訓練
６、アルゴリズムをテスト
７、繰り返し、Deploy</p>

<p>データサイエンスの多くの他の側面（データラングリング）により、RとＰｙｔｈｏｎは両方とも機械学習を利用するために人気のある言語だ。BigMLのようなMLaasやサービスとして機械学習を提供するスタートアップもある。</p>

<p>この投稿の結論として、ゴミ入れ、ゴミ出し、そしてアルゴリズムの問題でなければ、データ品質の問題が多い。データ量やアルゴリズムの複雑さは品質の代わりには成り得ない。そしてもちろん、全ての予見、機械学習は間違いうるということだ。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/24/why-spark-is-the-big-data-platform-of-the-future/">Why Spark Is the Big Data Platform of the Future</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-24T13:39:40+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>24</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>1:39 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><h3>原文</h3>

<p>Why Spark is the big data platform of the future
Posted on March 23, 2015 | 1 Comment
Apache Spark has created a lot of buzz recently. In fact, beyond the buzz, Apache Spark has seen phenomenal adoption and has been marked out as the successor to Hadoop MapReduce.</p>

<p>Google Trends confirms the hockey stick like growth in interest in Apache Spark.  All leading Hadoop vendors, including Cloudera, now include Apache Spark in their Hadoop distribution.</p>

<p>So what exactly is Spark, and why has it generated such enthusiasm? Apache Spark is an open-source big data processing framework designed for speed and ease of use.  Spark is well-known for its in-memory performance, but that has also given rise to misconceptions about its on-disk abilities. Spark is in fact a general execution engine – which has a greatly improved performance both in-memory as well as on-disk, when compared with older frameworks like MapReduce. With its advanced DAG (directed acyclic graph) execution engine, Spark can run programs up to 100x faster than MapReduce in memory, or 10x faster on-disk. Why is Spark faster than MapReduce?</p>

<pre><code>A key step during MapReduce operations is the synchronization or  “shuffle” step, intermediate between the “map”-step and the “reduce”-step. Apache Spark implements a sort-based shuffle design, which improves performance.
Apache Spark also includes a DAG (directed-acyclic graph) which allow developers to execute DAGs all at once, not step by step. This eliminates the costly synchronization required by MapReduce. Note that DAGs are also used by Storm and Tez
Spark supports in-memory data sharing across DAGs, so different jobs can work with the same data at a very high speed.
</code></pre>

<p>It’s important to remember that Hadoop is a decade-old technology, developed at a time when memory was still relatively expensive, and therefore took the design approach of persistence to disk as a way of maintaining state between execution steps. On the other hand, Spark was developed at UC Berkeley AMPLab in 2009 and then it was open-sourced in 2010 – when memory had become much cheaper. Therefore, Spark stores data in memory and transparently persists it to disk if needed, thereby achieving better performance. The core concept of Spark is this programming abstraction over data storage – called RDDs (Resilient Distributed Dataset). Under the hood, Spark automatically distributes the data contained in RDDs across the cluster and parallelizes the operations performed on them.</p>

<p>The end result is that, on an average – the lines of code required to develop a distributed data processing program is much less in Spark, when compared with MapReduce. See more details on why Spark is the fastest open-source engine for sorting a petabyte. Clearly, faster execution has been one of the key reasons for the uptake of Spark, but Spark also provides further advantages. Similar to YARN, the upgrade of the Hadoop framework over the MapReduce-only version, Spark allows a wide range of workloads from batch to interactive and streaming. It reduces the burden of maintaining separate tools as in Hadoop – and provides APIs in Scala, Java, Python and SQL.  Spark can run over a variety of cluster-managers, including Hadoop YARN, Apache Mesos, and Spark’s own standalone scheduler. Spark components</p>

<p>Spark Core – provides basic functionality of Apache Spark, including RDDs and APIs to manipulate them. Spark SQL – A new component which replaces the older Shark (SQL on Spark) project, this package provides better integration with Spark Core, it allows querying data through SQL and HiveQL and supports many data sources from Hive tables, Parquet and JSON. Spark SQL also allows developers to intermix SQL queries with the code for data manipulations with RDDs in Python, Java, and Scala. It also provides fast SQL connectivity to BI tools like Tableau or QlikView.</p>

<p>Spark Streaming – based on micro-batching, this component enables processing of real-time streaming data. It uses DStreams, which are series of RDDs, to process real-time data. The Spark Streaming API is very similar to the Spark Core RDD APIs, making it easy for developers to reuse and adapt code for batch to interactive or real-time applications. MLlib – provides a library of machine learning algorithms including classification, regression, clustering, and collaborative filtering, as well as model evaluation and data import. GraphX – provides an API for graphs and graph-parallel computations and operators for manipulating graphs and a library of graph algorithms. The SparkR project aims to provide a light-weight front-end to use Apache Spark from R. Work is on to integrate SparkR into Spark. Recently, Spark has introduced a dataframe library with R/Pandas syntax for use across all of the Spark language APIs and an ML pipeline API which also integrates with data frames. Spark adoption is increasing manifold, boosted by increased third-part vendor support. Databricks – the company spun out of AMPLab by the creators of Apache Spark, now provides Spark as a service on the cloud – with its own Databricks Cloud – which is in private beta. The Databricks cloud is designed to support data science in the lab as well as in the factory – by creating polyglot notebooks (mix of Scala/Java/Python/SQL possible) and building production pipelines for ETL and analytics jobs. Tableau and MemSQL have provided Spark connectors, Altiscale now provides Spark in the cloud and machine learning vendors like Nube are building products like Reifier to perform entity resolution and de-duplication using Spark. ClearStory Data provides Spark-based data processing and analytics. There is also a fledgling community of packages for Apache Spark. Big data and data science projects are complex with an increasing diverse toolset which require massive integration efforts. Greater flexibility than that provided by MapReduce, capability to support a variety of workloads and a simpler, more unified ecosystem of tools which work out of the box on a general execution engine (Apache Spark) thus provide better simplicity than the complex zoo of Hadoop MapReduce projects. Together with SparkSQL and dataframes library, Spark democratizes access to distributed data processing beyond MapReduce programmers extending it to other developers and business analysts. Over and above, considering the fast performance of Spark, it is no wonder that Apache Spark continues to gain traction and looks all set to be the default framework for Big data processing in the near future. More info:</p>

<pre><code>Spark record for fastest sort of a petabyte 
Dataframes in SparkS
</code></pre>

<h3>翻訳</h3>

<p>なぜSparkが未来のビッグデータプラットフォームなのか</p>

<p>Apache Sparkが最近話題に上がってくることが多くなった。事実、そういった話題以上に、Apache SparkはHadoop MapReduceの後任として位置を確立し、また現象的な採用実績を誇っている。</p>

<p>グーグルトレンドはApache Sparkへの関心がホッケースティック成長であることを確認している。Hadoopベンダーをリードする全て、Clouderaを含め、今やApache Sparkを彼らのHadoop分散技術に組み込んでいる。</p>

<p>では、Apache Sparkとは一体何か。そして、なぜそんな熱心にもてはやされるのか？Apache Sparkはオープンソースのビッグデータ処理フレームワークで、使いやすさとスピードに特化してデザインされた。
Apache Sparkはその内部メモリーによるパフォーマンスでよく知られ、しかしそのディスク上での能力について思い違いもまた生じている。スパークは事実としてはMApReduceのような古いフレームワークと比べられた時には、on-diskでもin-memoryでもとても改善されたパフォーマンスを発揮する総合実行エンジンだ。
その高度な有効無閉路グラフ実行エンジンにより、Apache Sparkはin memoryで100x、on deskで10xのスピード以上でプログラムを走らせることが可能になっている。では、なぜApache SparkはMapReduceよりも速いのか。</p>

<p>・MapReduceの場合、MapとReduceのステップの中間に、シャッフルステップ、もしくは同期の運用がキーステップとして存在している。Apache Sparkはパフォーマンスを改善するためにSort-based shuffleのデザインを実装している。
・Apache Sparkは開発者にひとつづつではなく、一度にDAGsを実行させることがDAG (directed-acyclic graph)もまた実装している。これはMapReduceによって要求されるコストがかかる同期を取り除いてくれる。また、これはStormやTazでも使用されている。
・Apache SparkはDAGsを介してのin-memoryデータシェアリングをサポートするので、同時にハイスピードで同じデータ上で違うジョブの稼働が可能になっている。</p>

<p>Hadoopはメモリが依然として相対的に高価であったような10年前に開発された産物であることを思い出すことは重要であるし、それゆえ実行するステップの合間に保守状態の方法としてディスクに固執したアプローチのデザインを取っていた。
一方、Sparkは2009年にバークレー大学でAMPラボで開発され、メモリがかなり安くなった2010年にOpenSourceになった。それゆえ、Sparkはメモリにデータを貯蔵し、必要であればディスクへ見える状態で入れておくこともする。これにより、よりよいパフォーマンスを達成している。</p>

<p>SparkのCoreコンセプトはRDDs (Resilient Distributed Dataset)と呼ばれるデータストレージを覆うこのプログラミング抽象化にある。この屋根の元、Sparkはそれらの上で行われる平行した運用とクラスタを横断したRDDs (Resilient Distributed Dataset)の内部に含まれるを自動的に分散する。</p>

<p>【Work-count code in Spark&rsquo;s Python API】
file = spark.textfile(&ldquo;hdfs://&hellip;&rdquo;)
file.flatfile(lamba line: line.split())
.map(lamba word: (word, 1))
.reduceByKey(lambda a, b: a+b)</p>

<p>この最終結果は平均として、分散型のデータプロセスを行うプログラムを開発するのに要求されるコードの行はMapReduceと比較したときにSparkでは非常に少ない行となる。
もっと詳しい説明に関しては下記を参照。
<a href="https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html">https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html</a></p>

<p>明らかにSparkの採用のための主要な理由の一つとしては速さが挙げられるが、Sparkはさらなる利点を供給する。YARN、HadoopフレームワークのupgradeはMapReduceのみのバージョンと似ていて、Sparkはバッチから対話とストリーミングへと広い幅の作業を許容している。
これはHadoopのように別れたツールを保守するための負荷を減らしてくれ、SQLやPython, Java,ScalaのAPIを提供してくれる。SparkはApache Mesos、Haddop YARN、そしてSpark自身のスタンドアローンスケジューラ－を含む様々なクラスタマネージャーをくるみ稼働する。</p>

<p>Spark Components</p>

<p>Spark-components</p>

<p>Spark Core- Apache SparkのRDDsと、それを操作するためのAPIを含む基本的な機能
Spark SQL- 古いShark(SQL on Shark)のプロジェクトにとって代わる新しいコンポネント。このパッケージはShark Coreとのより良い統合を提供するパッケージで、ParquetやJSON、Hivのテーブルからの多くのデータソースをサポートし、SQLとHiveQLをとおしてデータをクエリすることを許容する。
Spark SQLは開発者にPythonとJava、Scalaの内部でRDDsによるデータ操作のためのコードとSQLのクエリを混ぜることもさせる。これはまたTableauやQlick ViewなどのBItoolへの速いSQL接続を提供する。
Spark Streaming- マイクロバッチングをベースにして、このコンポネントはリアルタイムデータストリーミングのプロセスを利用可能としている。これはリアルタイムデータプロセスをするために、RDDsのシリーズであるDStreamsを使用している。
Spark Streaming APIは開発者にリアルタイムアプリ、もしくは対話形式のアプリへのバッチのコードを再利用、改変を簡単にするSpark Core RDD APIsにとても似ている。
MLib- 回帰、クラスタリング、分類、そして協調フィルタリング、データインポートとモデル評価をも含む機械学習のためのアルゴリズムライブラリを供給する。
GraphX- グラフアルゴリズムのライブラリとグラフを操作するためのグラフ平行計算と演算子、グラフ自身のためのAPIを提供する。
このSparkRプロジェクトはRからApache Sparkを使用するための軽いフロントエンドを提供することを目途としている。最近、SparkはR/Pandasのシンタクスでデータフレームライブラリを導入していて、R/Pandasのシンタクスは全てのSpark言語 APIと ML パイプラインAPIを通して使用でき、これらもまたデータフレームによって統合されている。Sparkの採用は多方面に広がっており、サードパーティーのベンダーサポートの増加によって加速している。Apache Sparkの製作者達によるAMPLabからのスピンアウト会社、Databricksは現在、Closedのベータ版    であるDatabricks自身のクラウド上でクラウドサービスとしてSparkを提供している。Databricksクラウドは多言語ノートブックス（可能なScala,Java,Python,SQLの混合）の制作と、ETLと分析の仕事のための製品経路をたてることによって、工場やラボでのデータサイエンスをサポートするためにデザインされた。TableauとMemAQLはSparkコネクタを提供し、Altiscaleは現在クラウドでSparkを提供している。そしてNubeのような機械学習のベンダーはSparkを使用してentity resolutionと重複削除を行うためのReifierのような製品を打ち立てている。Clear Story DataはSparkを基礎としたデータプロセスと分析を提供している。ここにはまた生まれたてのApache Sparkのためのパッケージのコミュニティーもまたある。ビッグデータとデータサイエンスのプロジェクトは巨大な統合のための努力が必要な多様化したツールセットの増加により、複雑だ。MapReduceによって供給されるもより素晴らしい柔軟性、作業と簡単さのバラエティをサポートする許容性、一般実行エンジン上のボックスを動かす、より一元化されたツールの環境システム。それゆえ、Hadoop MapReduceプロジェクトの複雑な動物園よりも、よりよい単純さというものを提供している。SparkSQLとデータフレームライブラリとともに、MapReduceのプログラマーだけでなく、ビジネスアナリストや他のプログラマーたちにも分散データプロセスへのアクセスへの参政権をSparkは 与えている。以上より、Sparkの速いパフォーマンスを考慮すると、近い将来にビッグデータプロセスのための全てのデフォルトフレームワークに、そして牽引し続けていっても不思議ではない。</p>

<p>詳細情報
<a href="https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html">https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html</a>
<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes">https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/24/An%20introduction%20to%20Data%20Science/">An Introduction to Data Science</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-24T13:36:54+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>24</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>1:36 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><p>I presented a talk last week introducing Data Science and associated topics to some enthusiasts.
Here’s a slide deck I created quickly with markdown using Swipe – a start-up building HTML5 presentation tools.
Here are the slides: <a href="https://www.swipe.to/2675ch">https://www.swipe.to/2675ch</a></p>

<p>データサイエンスへの導入
私は先週データサイエンスの導入と関連したトピックに関して熱心な人たちにプレゼンをした。
マークダウン記述でスワイプを使って即興で作成したスライドデッキをここに置いておく。スワイプはHtml5のプレゼンテーションツールを作成したスタートアップだ。
Here are the slides: <a href="https://www.swipe.to/2675ch">https://www.swipe.to/2675ch</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/21/data-science-project-life-cycle/">Data Science Project Life-cycle</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-21T14:49:44+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>21</span><span class='date-suffix'>st</span>, <span class='date-year'>2017</span></span> <span class='time'>2:49 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"></div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/07/21/data-science-project-life-cycle/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/19/what-roles-do-you-need-in-your-data-science-team/">What Roles Do You Need in Your Data Science Team?</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-19T13:10:55+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>19</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>1:10 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"></div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/07/19/what-roles-do-you-need-in-your-data-science-team/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/18/study-and-memo-about-apatche-spark/">Study and Memo About Apatche Spark</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-18T18:36:43+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>18</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>6:36 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="https://www.youtube.com/watch?v=1DAm9Cr23bg">https://www.youtube.com/watch?v=1DAm9Cr23bg</a></p>

<h3>PyConJPの動画でApatche Sparkの勉強メモ</h3>

<p>・Apatche Sparkは高速分散処理システム。MapReduceと比較すると、中間ファイル作成などがない。<br/>
・多目的なライブラリがあります。 -SparkSQLなど<br/>
・APIが多様です。</p>

<p>DeltaCubeではApathce Sparkで自動化し、クラスタリングしています。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/18/data-processing-with-spark-in-r-and-python/">Data Processing With Spark in R &amp; Python</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-18T14:57:39+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>18</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>2:57 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><h3>原文</h3>

<p>I recently gave a talk on data processing with Apache Spark using R and Python. tl;dr – the slides and presentation can be accessed below:<br/>
<a href="https://www.brighttalk.com/webcast/9059/172833">https://www.brighttalk.com/webcast/9059/172833</a></p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2017/07/18/data-processing-with-spark-in-r-and-python/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/18/translation1/">Translation1</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-18T14:56:11+09:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>18</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>2:56 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content">
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/2">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    
  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Souichi Narumiya -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  






<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/ja_JP/all.js#appId=1735411246487943&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
