<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[BI blog 4U]]></title>
  <link href="http://narusou.github.io/atom.xml" rel="self"/>
  <link href="http://narusou.github.io/"/>
  <updated>2017-07-21T15:09:17+09:00</updated>
  <id>http://narusou.github.io/</id>
  <author>
    <name><![CDATA[Souichi Narumiya]]></name>
    <email><![CDATA[narumiyasouichi@yahoo.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data Science Project Life-cycle]]></title>
    <link href="http://narusou.github.io/blog/2017/07/21/data-science-project-life-cycle/"/>
    <updated>2017-07-21T14:49:44+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/21/data-science-project-life-cycle</id>
    <content type="html"><![CDATA[<!--more-->


<h1>原文</h1>

<h3>The data science project lifecycle</h3>

<p>How does the typical data science project life-cycle look like?
This post looks at practical aspects of implementing data science projects. It also assumes a certain level of maturity in big data (more on big data maturity models in the next post) and data science management within the organization. Therefore the life cycle presented here differs, sometimes significantly from purist definitions of ‘science’ which emphasize the hypothesis-testing approach. In practice, the typical data science project life-cycle resembles more of an engineering view imposed due to constraints of resources (budget, data and skills availability) and time-to-market considerations.
The CRISP-DM model (CRoss Industry Standard Process for Data Mining) has traditionally defined six steps in the data mining life-cycle. Data science is similar to data mining in several aspects, hence there’s some similarity with these steps.</p>

<p>The CRISP model steps are:
1. Business Understanding
2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation and
6. Deployment</p>

<p>Given a certain level of maturity in big data and data science expertise within the organization, it is reasonable to assume availability of a library of assets related to data science implementations. Key among these are:</p>

<ol>
<li>Library of business use-cases for big data/ data science applications</li>
<li>Data requirements – business use case mapping matrix</li>
<li>Minimum data quality requirements (test cases to ensure minimum level of data quality to ensure feasibility)</li>
</ol>


<p>In most organizations, data science is a fledgling discipline, hence data scientists (except those from actuarial background) are likely to have limited business domain expertise – therefore they need to be paired with business people and those with expertise in understanding the data. This helps data scientists gain or work together on steps 1 and 2 of the CRISM-DM model – i.e. business understanding and data understanding.The typical data science project then becomes an engineering exercise in terms of a defined framework of steps or phases and exit criteria, which allow making informed decisions on whether to continue projects based on pre-defined criteria, to optimize resource utilization and maximize benefits from the data science project. This also prevents the project from degrading into money-pits due to pursuing nonviable hypotheses and ideas.</p>

<p>The data science life-cycle thus looks somewhat like:</p>

<ol>
<li>Data acquisition</li>
<li>Data preparation</li>
<li>Hypothesis and modeling</li>
<li>Evaluation and Interpretation</li>
<li>Deployment</li>
<li>Operations</li>
<li>Optimization</li>
</ol>


<p>Data Acquisition – may involve acquiring data from both internal and external sources, including social media or web scraping. In a steady state, data extraction and transfer routines would be in place, and new sources, once identified would be acquired following the established processes.</p>

<p>Data preparation – Usually referred to as “data wrangling”, this step involves cleaning the data and reshaping it into a readily usable form for performing data science. This is similar to the traditional ETL steps in data warehousing in certain aspects, but involves more exploratory analysis and is primarily aimed at extracting features in usable formats.</p>

<p>Hypothesis and modeling are the traditional data mining steps – however in a data science project, these are not limited to statistical samples. Indeed the idea is to apply machine learning techniques to all data. A key sub-step is performed here for model selection. This involves the separation of a training set for training the candidate machine-learning models, and validation sets and test sets for comparing model performances and selecting the best performing model, gauging model accuracy and preventing over-fitting.</p>

<p>Steps 2 through 4 are repeated a number of times as needed; as the understanding of data and business becomes clearer and results from initial models and hypotheses are evaluated, further tweaks are performed. These may sometimes include Step 5 (deployment) and be performed in a pre-production or “limited” / “pilot” environment before the actual full-scale “production” deployment, or could include fast-tweaks after deployment, based on the continuous deployment model.</p>

<p>Once the model has been deployed in production, it is time for regular maintenance and operations. This operations phase could also follow a target DevOps model which gels well with the continuous deployment model, given the rapid time-to-market requirements in big data projects. Ideally, the deployment includes performance tests to measure model performance, and can trigger alerts when the model performance degrades beyond a certain acceptable threshold.
The optimization phase is the final step in the data science project life-cycle. This could be triggered by failing performance, or due to the need to add new data sources and retraining the model, or even to deploy improved versions of the model based on better algorithms.
Agile development processes, especially continuous delivery lends itself well to the data science project life-cycle. As mentioned before, with increasing maturity and well-defined project goals, pre-defined performance criteria can help evaluate feasibility of the data science project early enough in the life-cycle. This early comparison helps the data science team to change approaches, refine hypothesis and even discard the project if the business case is nonviable or the benefits from the predictive models are not worth the effort to build it.</p>

<h1>翻訳</h1>

<h3>データサイエンスプロジェクトライフサイクル</h3>

<p>典型的なデータサイエンスプロジェクトのライフサイクルはどのようなものか？
この投稿では実装のデータサイエンスプロジェクトの実践的な側面をみていく。次の投稿においてはビッグデータ習熟モデルについて投稿するが、本投稿では習熟モデルと組織内でのデータサイエンスマネジメントのある水準を仮定していく。
それゆえ、ここでプレゼンされるライフサイクルは、仮説検証を強調する科学の純粋な定義とは時として大幅に異なる。
実践の中では、この典型的なデータサイエンスプロジェクトのライフサイクルは製品化までの完了期限への考慮、予算やデータと能力可用性のリソースに制限されたエンジニアの視点に共通項を持ち合わせる。</p>

<p>このCRISM-DMモデル（CRoss Industry Standard Process for Data Mining）はデータマイニングのライフサイクルの中で伝統的に６つのステップに定義されている。データサイエンスは何点かの側面からデータマイニングに似ている、それゆえそこにはこれらのステップにおいてもいくつかの類似点がある。</p>

<p>【CRISM-DMモデルのステップ】<br/>
１、ビジネスへの理解<br/>
２、データへの理解<br/>
３、データの準備段階<br/>
４、モデリング<br/>
５、評価<br/>
６、ディプロイ</p>

<p>組織でのデータサイエンスの専門知識とビッグデータの習熟度の水準さえあれば、データサイエンスの実装に関係したアセットライブラリの可用性を仮定することはリーズナブルだといえる。</p>

<p>【Key points】<br/>
１、ビッグデータとデータサイエンスのアプリケーションに関してのビジネスユースケースのライブラリ<br/>
２、データの要求-ビジネスユースケースのマッピングマトリクス<br/>
３、最低限のデータ品質の要求（実行可能性を保障するデータ品質の最低限の水準を保証するテストケース）</p>

<p>ほとんどの組織において、データサイエンスは生まれたての分野であり、結果としてデータサイエンティストたちは（数理のバックグラウンドを持っている者たちをのぞき）ビジネスについては専門知識が制限されているし、それゆえ彼らはビジネス側の人と組み、データに精通した専門知識とビジネス知識をペアにする必要がある。
これはデータサイエンティストがCRISM－DMモデルのビジネス理解、データ理解といった１、２のステップに進むのを手助けし、協働することを手助けする。</p>

<p>事前に定義された基準を基礎としてプロジェクトをつづけるかどうかの意思決定をさせるような、リソース利用の最適化とデータサイエンスプロジェクトの有益性を最大化するような完了標準と、フェーズやステップの定義されたフレームワークに関して、この典型的なデータサイエンスのプロジェクトはエンジニアリングの実践演習になる。
これはまたプロジェクトがあり得ない仮説と考えを追いかける金食い虫に成り下がることを防止する。
このデータサイエンスのライフサイクルはそれゆえ、いくらか下記のように見える。</p>

<p>１、データ獲得<br/>
２、データ準備<br/>
３、仮説とモデリング<br/>
４、評価と解釈<br/>
５、ディプロイ<br/>
６、運用<br/>
７、最適化</p>

<p>データ獲得-ウェブスクレイピングやソーシャルメディアを含む内部と外部のデータソース、両方からのデータ獲得を意味している。安定した状態では、データのエクストラクションと移動のルーティンが実行され、新しいソースが確認され次第、打ち立てられたプロセスに従って取得される。</p>

<p>データ準備-データラングリングとして参照され、データのクリーニングとデータサイエンスのパフォーマンスのために容易に利用できる形に落とし込むステップを含んでいる。</p>

<p>これはデータウェアハウスの側面における伝統的なETLのステップに似ているが、より予備的な分析と主に利用可能なフォーマットの中ではエクストラクトの特徴を狙いとしている。</p>

<p>仮説とモデリング-これは伝統的なデータマイニングのステップであり、しかしながらデータサイエンスのプロジェクトではこれらは統計的なサンプルデータに限定されていない。実際には、この考えは機械学習の技術をすべてのデータに適用するためにある。カギになるサブステップがここでモデルの選択のためになされる。<br/>
これは機械学習のモデル候補のトレーニングセットの分割を含み、また最高のパフォーマンスを出すモデルパフォーマンスの選択とモデルの正確さを図り、過学習を防ぐ。</p>

<p>ステップ２から４は必要に応じて何回も繰り返される、というのもビジネスとデータの理解がよりクリアになり、最初のモデルと仮説から評価され、さらに微調整も行われるためだ。継続移設モデル（continuous deployment model）をもとに、これらは時折、実際のフルスケールの本番環境にdeploymentされる前に、限定された、もしくはパイロット環境、プレ本番環境でステップ5（deployment）を行うことも含み、deploymentのあとの即時調整も含みうる。</p>

<p>一度このモデルが製品にdeploymentされると、定常業務と保守の時間がくる。この運用の段階もまた、ビッグデータのプロジェクトでの急な製品化への期限要求がくれば、継続移設モデル（continuous deployment model）と相性のいい対象のDevOpsに従いうる。理想的には、このdeploymentはモデルパフォーマンスをはかるためのパフォーマンステストを含み、このモデルパフォーマンスが許容できる閾値を超えて程度が低い時にはアラートを鳴らすことが出来ることだ。</p>

<p>最適化の段階はデータサイエンスのライフサイクルにおいて最後の段階になる。この段階はパフォーマンスの落ち込みにより、もしくは新しいデータソースが追加されたためモデルの再トレーニングをしなければならないことにより、またはよりよいアルゴリズムを基にした改良されたより良いモデルのversionをdeployするために使われる。</p>

<p>アジャイルのdeploymentプロセス、特に継続的なデリバリはそれ自身の多くをデータサイエンスプロジェクトのライフサイクルに貸し出している。以前にお話ししたように、熟練され、良く定義されたプロジェクトのゴール、事前に定義されたパフォーマンスの標準はそのライフサイクルの中の十分早い段階で実行可能性を評価することを手助けすることができる。この比較はデータサイエンスチームがアプローチを変更したり、仮説を再精製したり、予見されるモデルからの利益が組み上げる価値のないものだったりビジネスケースが育成不能なものであれば、プロジェクトを廃棄することでさえ早い段階で助けになる。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Roles Do You Need in Your Data Science Team?]]></title>
    <link href="http://narusou.github.io/blog/2017/07/19/what-roles-do-you-need-in-your-data-science-team/"/>
    <updated>2017-07-19T13:10:55+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/19/what-roles-do-you-need-in-your-data-science-team</id>
    <content type="html"><![CDATA[<!--more-->


<h1>原文</h1>

<p>Over the past few weeks, we’ve had several conversations in our data lab regarding data engineering problems and day to day problems we face with unsupervised data scientists who find it difficult to deploy their code into production.</p>

<p>The opinions from business seemed to cluster around a tacit definition of data scientists as researchers, primarily from statistics or mathematics backgrounds, who are experienced in machine learning algorithms and often in some domain areas specific to our business, (e.g. actuaries in insurance), but not necessarily having skills of writing production-ready code.
The key driver behind the somewhat opposing strain of thought came from the developers and data engineers who often quoted Cloudera’s Director of Data Science – Josh Wills – famous for his “definition of a data scientist tweet”:
“Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician.”</p>

<p>Josh Wills’ definition of data scientist</p>

<p>Wills’ quote reflects the practical issues in finding “unicorn” data scientists and having to do with the best of what’s on offer for a multi-disciplinary area like data science. It’s also perhaps based on his work in startups like Cloudera and web giants like Google, where adopting agile practices like DevOps allow data scientists closer interaction with engineers and therfore substantial experience in deploying to production. Unfortunately, that’s always a challenge due to bureaucracy, mindset, lack of informed opinion and cultural barriers in larger or old-world organizations with legacy systems and practices.</p>

<p>As in any startup or lab working on problems in data science and big data, it’s important for us to clear misconceptions and get the team to a shared understanding of commonly used terms to establish a foundational common language, which would then allow developing a shared vision around our objectives. Therefore it’s necessary to review going beyond definitions of the “unicorn” data scientists and looking at what happens in real-life teams where data scientists work, like ours.</p>

<p>Different perspectives
A lot of the data scientists actually think of themselves as mathematicians, trying to formulate business problems into math/statistics problems and then trying to solve them in the data science projects.
However, the popular misconception arise sometimes out of the big-data hype articles churned out by big data vendors, including some evangelists – who equate data scientists with superpowers across a multitude of disciplines.
The developer’s views arise due to their unique perspectives on the complexities of data wrangling and fragmentation around tools, technologies and languages.</p>

<p>The reality, as always, is quite different from the hype. There are actually probably just a handful of the “unicorn” data scientists on the planet, who have superpowers in maths/stats,AI/machine learning, a variety of programming languages, an even wider variety of tools and techniques, and of course are great in understanding business problems and articulating complex models and maths in business-speak. For the lesser mortals, and less fortunate businesses, we have to do with multiple individuals to combine these skillsets together into a team or data science squad.</p>

<p>Building data science teams</p>

<p>In terms of hiring, building a data science team becomes much easier, once we get around the idea that the “unicorn” data scientists are not really available. The recruitment team and hiring manager can then focus on the individual skills that are required on the team and try to hire for profiles with strengths in these skills. Once hired, the manager’s role switches to building the team in terms of setting expectations and facilitating collaborative team dynamics to evolve self-governing teams, which can then focus on achieving the objectives in a collaborative manner, instead of having to be superheroes.
Dream data science team?</p>

<p>The roles in a data science team</p>

<p>So what roles would a data science team have? Depending upon the organizations’ objectives, the team could either focus on service-oriented consulting gigs, or focus on building reusable assets or data products.</p>

<pre><code>Data scientist – this role would be primarily of someone who can work on large datasets (usually on Hadoop/Spark) with machine learning algorithms, develop predictive models, understand the “theory” – maths and stats behind the models and can interpret and explain model behavior in jargon-free language. Typically this role requires good knowledge of SQL and familiarity with at least one programming language for predictive data analysis e.g. R and/ Python.
</code></pre>

<p>Netflix requirements for data scientist role</p>

<pre><code>Data engineer / Data software developer – this role is for someone who has good understanding of distributed programming, including insfrastructure and architecture. Typically this person is comfortable with installation of distributed programming frameworks like Hadoop MapReduce/Spark clusters, can code in more than one programming languages like Scala/Python/Java, and knows Unix scripting and SQL. Based on range and depth of experience, this role can evolve into one of the two specialized roles – that of the data solutions architect and the data platform administrator.
</code></pre>

<p>Netflix requirements for data engineer role</p>

<pre><code>Data solutions architect – A data engineer with a range of deep experience across several distributed technologies, and who also has good understanding of service-oriented architecture concepts and web applications (SOA concepts and REST frameworks) in addition to the developer skillsets.
Data platform administrator – A data engineer who has extensive experience across distributed technologies, especially managing clusters including production envionments and good knowledge of cloud computing architectures (public clouds like AWS if using public cloud or OpenStack and Linux sysadmin experience if using private/hybrid clouds)
Full-stack developer – This is an optional role – only required for teams which are focused on building data products with web interfaces. The full-stack developer is ideally an experienced web developer with experience in both backend and front-end e.g. a MEAN developer with experience on MongoDB, Express, AngularJS and NodeJS.
Designer – this role demands an expert who has deep knowledge of user experience (UX) and interface design, primarily for web/mobile applications depending on target form factors of the data product as well as data visualization and desirably some UI coding expertise. Building quick mockups and wireframes design is often required during product definition, and the designer needs to be able to work with business as well as developers in a collaborative fashion. Sometimes this role is played by front-end UI developers as good designers don’t come cheap.
</code></pre>

<p>Netflix requirements for UX designer</p>

<pre><code>Product manager – This is an optional role – only required (but the key one) for teams focused on building data products. Defining the product vision, translating business problems into user stories, and focusing on getting the development team to build data products based on the user stories, aligning product releases and overall roadmap to business requirements and expectations is a key requirement from this role. Having product management experience along with relevant technical expertise is critical for this role due to differences in life-cycles of products and IT projects, as also the ability to present the voice of the customer and balance long-term vision with short-term needs. Back-filling this role with data scientists/developers who do not have product vision/business acumen is dangerous due to lures of gold-plating and lack of project management skills.
</code></pre>

<p>Google product manager role</p>

<p>Google requirements for product manager</p>

<pre><code>Project manager role may also be optionally required when the team is low on experience. In most successful cases of performance, managers set the objectives and expectations and facilitate to build self-governing teams following agile practices.
</code></pre>

<p>Irrespective of whether the data science teams focus on consulting services in one-off projects or build data products which are reused, in both cases, the team would still require a minimum foundation to build on – in terms of processes or shared understanding, and tools and platforms to perform the actual work. We’ll review the data engineering requirements for such tools and platforms in the next post.</p>

<h1>翻訳</h1>

<h3>データサイエンスチームにはどんな役割が必要か？</h3>

<p>数週間前に私達のデータラボで、管理されていないデータサイエンティストがproductionにコードをdeployするのに困難を極めているといった、日々の問題やエンジニアリングの問題に関して何度となく話し合いが行われた。
統計や数学と言ったリサーチ畑のバックグランドから、ビジネス側の意見としてはデータサイエンティストの暗黙の了解の周りに集まったようだ。というのも、彼らは機械学習のアルゴリズムを経験していたり、我々の分野に特定されるような（保険の数理等）、しかしコードを読む必要がない内容の業務経験していた。
いくらか考えの歪みに向かっていくKey DriverはClouderaのデータサイエンスのディレクター（Jhon Wills）をよく引用する。(データサイエンティストの定義のツイートで有名)</p>

<p>&lsquo;データサイエンティスト（名詞）：統計を行う人材よりもソフトエンジニアに精通し、ソフトエンジニアを行う人間よりも統計に精通した人材&rsquo;</p>

<p>ウィルの引用は「幻のユニコーン」データサイエンティストを探すことに、そしてデータサイエンティストのような複数分野における修練が必要な仕事において最高のものに携わらねばならないといった実践的な問題を浮かび上がらせる。これらの知見は、おそらくはデータサイエンティストとエンジニアにより近い相互関与をさせるDevopsのようなAgileの実践を取り入れ、またそれゆえＰｒｏｄｕｃｔｉｏｎにDeployするといった実質的な経験をさせるウェブジャイアント：GoogleやClouderaとい ったスタートアップに基 礎をお いている。不幸なことにそういうものはレガシーシステムと塩漬けの時代遅れの組織、もしくは大組織の官僚制やマインドセット、詳しい見解の欠如、文化的な隔たりを理由にして常に挑戦的である。</p>

<p>ビッグデータとデータサイエンスの分野の問題にスタートアップやラボが取り組んでいるのだが、誤認をクリアにすることや我々の目標の周辺のシェアされたヴィジョンを培うことを可能にする基本的な共通言語を打ち立てるために、よく使われる用語の理解をチームでシェアすることは我々にとって重要な事柄だ。
それゆえ、「幻のユニコーン」データサイエンティストの定義をこえて、また我々のチームのようなデータサイエンティストが働く実際のチームで何が起きているのかを見直すことが必要である。</p>

<h3>異なる知見</h3>

<p>データサイエンティストの多くが自らのことを数学者として考えている、というのもビジネスの問題を数学/統計学の問題の中にまとめようとし、またその時にはデータサイエンスプロジェクトの内部でその問題を処理しようとする。しかしながら、よくある誤認がビッグデータのベンダーや修練の賜物としての超能力とデータサイエンスを同一視するようなエヴァンゲリストによって大量に刷られた誇大広告の記事外部から時々立ち上ってくる。開発者の視点はデータのETL、ツールの断片化、技術と言語の複雑さの上のユニークな認識から生じている。</p>

<p>現実はいつも、誇大広告とはかなり異なる。そこには実はおそらく、ただ一握りの数学、統計学、AIと機械学習、さまざまなプログラミング言語、そしてさらに幅広い様々なツールと技術、そして当然ビジネスの問題の理解とビジネスの話題の中で複雑なモデルと数学を明確に述べるような超絶の能力をもつ「幻のユニコーン」データサイエンティストがその惑星の上にいるだけなのだ。低い能力者、そして運に見放されたビジネス、私たちはこういった複数の個人たちとこれらのスキルセットをチーム、もしくはデータサイエンス部隊に組み込まなくてはならないのだ。</p>

<h3>データサイエンティストのチームを作る</h3>

<p>雇用に関し、「幻のユニコーン」のデータサイエンティストが本当は使用し得るものではないという考えの周辺に辿り着けば、データサイエンティストのチームを作成するのはかなり簡単になる。リクルートのチームとそのマネージャーはその時、チームの上で求められる個別のスキルに焦点を充てられる、またそれらの個別のスキルに強いプロフィールを雇おうとすることもできる。一度雇えば、マネージャーの仕事はスーパーヒーローになる代わりに、期待されることの設定や協力的なマナーの上で目標を達成することに焦点を当てられるような動的に自己で政治的に治められたチームに進歩するように仲介し調整するようにチームビルディングをすることへ役割が変わっている。</p>

<h3>データサイエンスチームの役割</h3>

<p>では、データサイエンスのチームはどんな役割があるのだろうか。組織の目標に依存し、そのチームはサービス志向のコンサルタントの仕事、また再利用可能な資産、もしくはデータ製品を打ち立てることに焦点を当てる。</p>

<h3>データサイエンティスト</h3>

<p>この役割は機械学習アルゴリズム、予見しうるモデルの開発、そのモデルの背後にある数学と統計の理論を理解し専門用語なしでの解釈、説明ができる誰かが優先的になるだろう。
典型的にこの役割はSQLの能力と例えばRやPythonのような予見データ分析に使用するプログラミング言語を少なくとも一つに秀でている。</p>

<p>&lsquo;資格：統計学、または数学のPhDもしくはMasterの学位、オペレーションリサーチ、CS、もしくは経済学やその他の関係のある分野<br/>
5年以上のビジネス上の重要な影響をきりもりするための分析を利用した追跡可能な証明されうる適切な経験値<br/>
SQLに特化<br/>
統計学的な知識と直観力 理想的にはABテストを利用<br/>
ファイル分散データベースの経験、Hive、もしくはPigのようなクエリ言語の使用経験<br/>
パイソン、ルビー、パールのようなスクリプト言語のプログラミング経験<br/>
情報と結果をクリアに提示するための強力なデータ視覚化能力<br/>
RやSASといった統計分析ツールの熟練度  &rsquo;</p>

<h3>データエンジニア・データソフトウェアエンジニア</h3>

<p>この役割はインフラの理解とアーキテクチャの理解を含んだ分散処理のプログラミングに対して習熟している人物にあてられる。</p>

<p>典型的にはunixスクリプトとSQLをしっていて、Java/Python/Scalaのようなプログラミング言語を一つ以上コードすることが出来、Haddop/MapReduce/Spark のクラスタのような分散型プログラムのフレームワークの環境をよしとする人物だ。
経験の深さと幅を基礎として、この役割は特別な二つの役割に転化する。データプラットフォームの管理者とデータソリューションアーキテクトがその二つだ。</p>

<p>&lsquo;成功に導く技術的な経験（厳格な試金石としてではなく）：Netfilixのデータエンジニア<br/>
mapReduce/Hive/Pig/Sparkその他の経験<br/>
Java, Python,Scalaをコードできるスキル<br/>
ビッグデータとETLの製品サポート経験<br/>
SQLにつよいこと<br/>
定量：機械学習もしくは統計学の基本的な理解を与えるような本、もしくは授業を受けたことがある。<br/>
app/web開発、ソフトウェアエンジニアの原則の理解<br/>
        　</p>

<h3>データソリューションアーキテクト</h3>

<p>いくつかの分散型の技術に関して深い経験のあるデータエンジニア、またサービス志向の設計コンセプトとウェブアプリケーション（SOAコンセプトとRESTフレームワーク）の理解、また加えて開発者としてのスキルセットをもつデータエンジニア</p>

<h3>データプラットフォーム管理者</h3>

<p>分散型の技術にかんして広範囲な経験をもつ。特に、本番環境をふくむクラスタをマネージした経験とクラウドコンピューティングの設計のよき理解（OpenStackもしくはパブリッククラウドを使用しているのであれば、AWSのようなパブリッククラウド。そしてもしもプライベート/ハイブリッドのクラウドであるならば、リナックスのシスアドの経験）
    　</p>

<h3>フルスタック開発者</h3>

<p>これは選択的な役割だ。ウェブインターフェースでのデータプロダクトの打ち立てに焦点を当てるチームにのみ必要な役割だからだ。フルスタックの開発者は理想的には例としてMongoDB, Express, AngularJS and NodeJSなどの経験の上、MEAN開発者のようなフロントエンドとバックエンド両方に精通した開発者が望ましい。</p>

<h3>デザイナー</h3>

<p>この役割はUXとインターフェースデザインに深く精通したエキスパートが望まれる。主に、データ視覚化、データ製品のターゲットフォームに依存したウェブ、モバイルのアプリケーションのUXデザインとUIコーディングの専門知識を有している。即座にモデルやワイヤーフレームをデザインする事が製品定義の中で求められ、チームワークの流れの中で開発者たち、またビジネス側とも協働して働くことが可能であることが必要になってくる。ときどきこの役割はデザインが得意なフロントエンドのUI開発者によって担われるが、安上りではない。</p>

<p>&lsquo;資格：必須項目として<br/>
5年以上のデザインに関する相互経験（TVのインターフェースなどは望ましい。）<br/>
素晴らしいポートフォリオ<br/>
アドビクリエイティブスーツのエキスパート（動的なものであれば望ましい）<br/>
複数のデザインを即座に清算できる技能<br/>
詳細とマルチタスクへの注意があるという明確な証明<br/>
あなたのプロジェクトをマネージするための修練されたアプローチ&rsquo;</p>

<h3>プロダクトマネージャー</h3>

<p>この役割はデータ製品を組み立てるためのチームにのみ、またそれがキーワードとして必要になる役割だ。製品のヴィジョンを定義し、ユーザーのストーリーの中にビジネスの問題を落とし込み、ユーザーのストーリーの上でデータ商品をつくるための開発チームをつくり、ビジネスの期待と要求への全体のロードマップと製品のリリースにアライメントを取ることといった内容がこの役割の主な要求になる。適切な技術的な専門知識に沿ったプロダクトマネジメントの経験はITprojectと製品のライフサイクルにおける差異を理由として必須になっている。また、短い期間のニーズと長期のヴィジョンのバランス、顧客の声をプレゼンする能力も必須になる。商品の観点とビジネスへの鋭い視点を持っていない開発者やサイエンティストでこの役割を埋め合わせるのはプロジェクトマネジメントのスキルの不足と金メッキの誘惑のせいで危険なものになる。</p>

<p>&lsquo;最低限の資格：コンピュータサイエンス、技術分野のBA/BSの学位、もしくは相当する経験<br/>
         5年以上の企業のソフト、もしくは消費者のインターネットサービスのプロダクトマネージャーとしての経験</p>

<p>望ましい資格：非常に複雑なビジネスロジックの大きな企業システム、もしくはその他のシステムの経験<br/>
       ビッグデータ技術、データウェアハウスのデータマネジメント経験<br/>
       協調性があり、クロスファンクショナルな環境における少々のインストラクションと監督によって結果を出すことに慣れているリーダー<br/>
       ソリューションを導くデータをデリバリするために、大きな量の情報で働く能力のデモ<br/>
       両義的な問題をクリアソリューションに飛躍させることが出来るようなイノベイティブで素晴らしい問題解決能力<br/>
            　　</p>

<h3>プロジェクトマネージャー</h3>

<p>この役割は経験に乏しいチームであるときには選択的に求められる。パフォーマンスとして最高のケースは、マネージャーは目標と期待を設定し、アジャイルプラクティスをフォローするような自治するチームに調整する。
    　　</p>

<p>one-offのプロジェクト、もしくは再利用される商品のコンサルティングサービスに焦点をおくデータサイエンスチームにかかわらず、このチームは依然として共通した理解、プロセスに関して、そしてツールと実際の仕事を行うためのプラットフォームとツールに関して最低限の基礎を必要とする。私たちはそういったツールやプラットフォームといったもののためのデータエンジニアリングするための必要項目を次の投稿で見直すこととする。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Study and Memo About Apatche Spark]]></title>
    <link href="http://narusou.github.io/blog/2017/07/18/study-and-memo-about-apatche-spark/"/>
    <updated>2017-07-18T18:36:43+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/18/study-and-memo-about-apatche-spark</id>
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=1DAm9Cr23bg">https://www.youtube.com/watch?v=1DAm9Cr23bg</a></p>

<h3>PyConJPの動画でApatche Sparkの勉強メモ</h3>

<p>・Apatche Sparkは高速分散処理システム。MapReduceと比較すると、中間ファイル作成などがない。<br/>
・多目的なライブラリがあります。 -SparkSQLなど<br/>
・APIが多様です。</p>

<p>DeltaCubeではApathce Sparkで自動化し、クラスタリングしています。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Processing With Spark in R &amp; Python]]></title>
    <link href="http://narusou.github.io/blog/2017/07/18/data-processing-with-spark-in-r-and-python/"/>
    <updated>2017-07-18T14:57:39+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/18/data-processing-with-spark-in-r-and-python</id>
    <content type="html"><![CDATA[<h3>原文</h3>

<p>I recently gave a talk on data processing with Apache Spark using R and Python. tl;dr – the slides and presentation can be accessed below:<br/>
<a href="https://www.brighttalk.com/webcast/9059/172833">https://www.brighttalk.com/webcast/9059/172833</a></p>

<!--more-->


<p>As noted in my previous post, Spark has become the defacto standard for big data applications   and has been adopted quickly by the industry. See Cloudera’s  One Platform initiative blog post by CEO Mike Olson for their commitment to Spark.　In data science R had seen rapid adoption, not only because it was open source and free   compared to costly SAS, but also the huge number of statistical and graphical packages provided　by R for data science. The most popular ones of course are the ones from Hadley Wickham (dplyr,　ggplot2, reshape2, tidyr and more). On the other hand, Python had seen rapid adoption among　developers and engineers due to its being useful to script big data tasks along with data　analysis with the help of packages like pandas, scikit-learn, NumPy, SciPy, matplotlib etc.　and also the popular iPython &amp; later Jupyter notebooks.　</p>

<p>There are numerous posts strewn on the net picking fights between R and Python. However it is quite usual for any big data and data science shop to have developers and data scientists who use either or both these tools. Spark makes it easy for both communities to leverage the power of Hadoop and distributed processing systems with its own APIs like DataFrames which can be used in a polyglot fashion. Therefore it is essential for any data enthusiast to learn about how data processing in Spark can be done using R or Python.</p>

<h3>翻訳</h3>

<p>最近、RとPythonを使用したApatche Sparkによるデータプロセスに関して話をした。下記のURLからスライドとプレゼンテーションにアクセス可能。</p>

<p><a href="https://www.brighttalk.com/webcast/9059/172833">https://www.brighttalk.com/webcast/9059/172833</a></p>

<p>過去の投稿にあるように、Sparkはビックデータアプリケーションの事実上のスタンダードになり、また急速にBI業界に取り入れられた経緯がある。CEO Mike Olsonによる彼らのSparkへのコミットメントに関するブログ投稿を参照。
データサイエンスに関して、RはコストがかかるSASに比べてフリーでオープンソースであるという理由のみならず、データサイエンスのために用意された統計的でグラフィカルな数多くのパッケージによって急速に普及してきた。もっともよく知られている事例としてはHadley Wickham(dplyr, ggplot2, reshape2, tidyr and more)による。一方、Pythonは開発者とエンジニアの間で、pandasやscikit-learn、NumPy, SciPy,matplotlibとかのようなパッケージによってデータアナリシスにおけるビッグデータのタスクを記述するのに便利であるため急速に採用されてきた。そして、iPython, 後期のJupiter notebooksによるものがよく知られている。</p>

<p>非常に多くのRとPythonに関する両社の批判や議事が散らばった投稿として見受けられる。しかし、これらを同時に両方、もしくは二種類を使用するデータサイエンティストと開発者達を保有するのはデータサイエンスやビッグデータを生業にするものにとってはかなり普通のことと思われる。SparkはHadoopの力と、多言語仕様で使用することが出来るデータフレームのようなSparkのAPIと、それを使用した分散処理プロセスシステムを、この両者のコミュニティが利用することを簡単にする。こういった背景から、RとPyhonによるSparkが行うデータプロセスがどのようなものなのかを学ぶことはデータマニアにとっては必要不可欠なことだと思われる。</p>

<h3>出典</h3>

<p><a href="https://biguru.wordpress.com/2015/11/18/data-processing-with-spark-in-r-python/">https://biguru.wordpress.com/2015/11/18/data-processing-with-spark-in-r-python/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Translation1]]></title>
    <link href="http://narusou.github.io/blog/2017/07/18/translation1/"/>
    <updated>2017-07-18T14:56:11+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/18/translation1</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Test]]></title>
    <link href="http://narusou.github.io/blog/2017/07/15/test/"/>
    <updated>2017-07-15T22:20:27+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/15/test</id>
    <content type="html"><![CDATA[<p>This site is created for my practice to develop homepages and traslate articles about Business Intelligence.</p>

<!--more-->


<p><img src="http://narusou.github.io/images/test.png" alt="test" /></p>

<h2>環境</h2>

<p>PC:HPchromebook<br/>
OS:Google Chrome and UBUNTU 14.04 LTS</p>

<h2>OctPressをGITHUBにdeployするためにしたこと</h2>

<pre><code>1,githubにブランチ作成
2,rbenvのインストール
3,rubｙのインストール
4,githubとlocalの同期
5,rakeで記事の作成
6,generate, deploy
</code></pre>

<h2>躓いた点</h2>

<p> ・rubyとrbenv versionの整理<br/>
 ・git pull</p>

<h2>使用したコード 順不同</h2>

<pre><code>$ rake deploy
$ rake generate
$ ruby -v
$ ruby --version
$ rbenv versions
$ rbenv version
$ source ~/.bash_profile
$ echo $PATH
$ which ruby
$ which rbenv
$ rbenv init
$ rbenv global 2.4.1
$ bundle exec 
$ git pull
$ git push
$ git commit
$ sudo apt-get install ruby
$ sudo apt-get install git
$ git clone git://github.com/sstephenson/rbenv.git ~/.rbenv
$ git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build
$ echo 'export PATH="$HOME/.rbenv/bin:$PATH"' &gt;&gt; ~/.profile
$ echo 'eval "$(rbenv init -)"' &gt;&gt; ~/.profile
$ exec $SHELL -l
$ rbenv install -l
$ rbenv install 2.4.1
$ rbenv exec gem install bundler
$ rbenv rehash
$ git clone git://github.com/imathis/octopress.git octopress
$ cd octopress
$ bundle install --path vendor/bundle
$ git clone git://github.com/lucaslew/whitespace.git .themes/whitespace
$ rake install['whitespace'] # for zsh, use: rake install\['whitespace'\]
$ rake new_post['test']
$ rake preview
$ rake setup_github_pages
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First_test]]></title>
    <link href="http://narusou.github.io/blog/2017/07/15/first-test/"/>
    <updated>2017-07-15T21:36:23+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/15/first-test</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
</feed>
