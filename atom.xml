<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[BI blog 4U]]></title>
  <link href="http://narusou.github.io/atom.xml" rel="self"/>
  <link href="http://narusou.github.io/"/>
  <updated>2017-07-29T01:32:24+09:00</updated>
  <id>http://narusou.github.io/</id>
  <author>
    <name><![CDATA[Souichi Narumiya]]></name>
    <email><![CDATA[narumiyasouichi@yahoo.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Brief Intoroduction to Statistics - Part1]]></title>
    <link href="http://narusou.github.io/blog/2017/07/24/a-brief-intoroduction-to-statistics-part1/"/>
    <updated>2017-07-24T13:46:42+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/24/a-brief-intoroduction-to-statistics-part1</id>
    <content type="html"><![CDATA[<!--more-->


<h3>原文</h3>

<p>A Brief Introduction to Statistics – Part 1
Posted on October 28, 2014 | Leave a comment
What is Statistics?
Collected observations are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data. Each observation in data is called a case. Characteristics of the case are called variables. With a matrix/table analogy, a case is a row while a variable is a column.
Statistics - Correlation
Statistics – Correlation (Courtesy: xkcd.com)
Types of variables:
Numerical– Can be discrete or continuous, and can take a wide range of numerical values.
Categorical– Specific or limited range of values, usually called levels. Variables with natural ordering of levels are called ordinal categorical variables.
A pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent.
Data collected in haphazard fashion are called anecdotal evidence. Such evidence may be true and veriﬁable, but it may only represent extraordinary cases.
There are two main types of scientific data collection:
Observational studies – collection of data without interfering with how the data has arisen. Can provide evidence of a naturally occurring association between variables, but by themselves, cannot show a causal connection.
Experiments – randomized experiments, usually with an explanatory variableand a response variable are performed, often with a control group.
In general, correlation does not imply causation, and causation can only be inferred from a randomized experiment.
Types of sampling:
Simple random sampling: Each subject in the population is equally likely to be selected.
Stratified sampling: The population is first divided into homogeneous strata (subjects within each stratum are similar, but different across strata) followed by random sampling from within each stratum.
Cluster sampling: The population is first divided into groups or clusters (subjects within each cluster are non-homogeneous, but clusters are similar to each other). Next a few clusters are randomly sampled followed by random sampling from within each cluster.
Randomized experiments are generally built on four principles:
Controlling – control any differences between groups for confounding variables which are known and can be accounted for.
Randomization – randomize population into groups to account for variables that cannot be controlled.
Replication – collect sufficiently large sample or replicate entire study to improve estimation.
Blocking – advanced technique of grouping population based on variable known/suspected to influence response, followed by randomizing cases within the group.
Reducing bias in experiments –
Randomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and eﬀect relationships in all cases. Blinding can help in overcoming placebo effect in human studies.
Distributions of a numerical variable are described by shape, center and spread. The three most commonly used measures of center and spread are:
center: mean (the arithmetic average), median (the midpoint), mode (the most frequent observation)
spread: standard deviation (variability around the mean), range (max-min), interquartile range IQR (middle 50% of the distribution)
An outlier is an observation that appears extreme relative to the rest of the data.
A robust statistic (e.g. median, IQR) is a statistic that is not heavily affected by skewness and extreme outliers.
Comparing categorical data:
A table that summarizes data for two categorical variables in this way is called a contingency table. A table for a single variable is called a frequency table. A bar plot is a common way to display a single categorical variable. A segmented bar plot is a graphical display of contingency table information. A mosaic plot is a graphical display of contingency table information that is similar to a bar plot for one variable or a segmented bar plot when using two variables. While pie charts are well known, they are not typically as useful as other charts in a data analysis.
Comparing numerical data:
The side-by-side box plot is a traditional tool for comparing across groups. Another useful plotting method uses hollow histograms to compare numerical data across groups.
Hypothesis test:
H0 Independence model – Explanatory variable has no eﬀect on response variable, and we observed a diﬀerence that would only happen rarely.
HA Alternative model – Explanatory variable has an eﬀect on response variable, and what we observed was actually due to explanatory variable effect on the response variable explaining the difference.
Based on the simulations, we have two options:
1. We conclude that the study results do not provide strong evidence against the independence model.
2. We conclude the evidence is suﬃciently strong to reject H0 and assert the alternative hypothesis.
When we conduct formal studies, usually we reject the notion that we just happened to observe a rare event. So in such a case, we reject the independence model in favor of the alternative.
Statistical Inference:
One ﬁeld of statistics, statistical inference, is built on evaluating whether such differences are due to chance. In statistical inference, statisticians evaluate which model is most reasonable given the data. Errors do occur, just like rare events, and we might choose the wrong model. While we do not always choose correctly, statistical inference gives us tools to control and evaluate how often these errors occur.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Designing the Future -Data Innovation LAbs]]></title>
    <link href="http://narusou.github.io/blog/2017/07/24/designing-the-future-data-innovation-labs/"/>
    <updated>2017-07-24T13:44:46+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/24/designing-the-future-data-innovation-labs</id>
    <content type="html"><![CDATA[<!--more-->


<h3>原文</h3>

<p>Designing the future – Data Innovation Labs
Posted on January 19, 2015 | Leave a comment
With the ongoing Big data revolution, and the impending Internet of Things revolution, there has been a renewed enthusiasm in “innovation” around data.  Similar to the Labs concept started by Google (think Gmail Beta based on Ajax, circa 2004), more and more organizations, business communities, governments and countries are setting up Labs to foster innovation in data and analytics technologies. The idea behind these “data innovation labs” is to develop avant-garde data and analytics technologies and products in an agile fashion and move quickly from concept to production. Given the traditional bureaucratic setup in large organizations and governments, these Labs stand a better chance of fostering a culture of innovation, due to their being autonomous entities and their startup-mode culture leveraging agile methodologies.
Data Innovation Labs
Data Innovation Labs
Below I list a few of the data innovation labs that have been setup to get value for their parent entities in the data and analytics space, trying to build data products in the Big Data and Data Science fields.</p>

<pre><code>Data Innovation Lab – Thomson Reuters
    Small group of about ten people, partnering with internal teams, third-parties and customers to find data-driven innovations
    experiments with mash-ups of internal and external data in novel ways
    hosts internal crowd-sourcing competitions
    translate business problem into technical data problem statement
    created Exchange – a digital forum for sharing ideas and insights
    Partners with Central Strategy to estimate potential and market-size for new data innovation opportunities
The Data Lab – Scotland
    mission is to strengthen Scotland’s local industry and transfer world-leading research in informatics and computer science in the global marketplace
    focuses on skills and training by working with industry  to create  a pipeline of talented data scientists equipped with the relevant skills
    connects world-leading researchers and data scientists with local industry and public sector organizations, giving them access to experts who can help collaborate on solutions to key problems
Smart Data Innovation Lab – German government
    Hosted at Karlsruhe Institute for Technology, its mission is to turn big data into smart data
    plans to store data centrally in a highly secured environment for research purposes
    has cutting-edge insfrastructure for processing Big Data including software like SAP HANA, IBM Watson and hardware on IBM Power and Intel architectures
    Industry partners to deliver data sources directly from the practice environment, to be complemented with crowdsourced data and open data
    plans to offer an open source repository for reuse in research
Midata Innovation Lab – UK government
    An organization run by the Department of  Business Innovation &amp; Skills with involvement of industry
    Accelerator for businesses to create new services for consumers, from data
    work involves concept of personal data stores (PDS) or personal clouds
    working with three leading PDS – Allfiled, Mydex and Paoga
    Participating organizations and developers use PDS to create new innovative services for consumers
Nordstrom Innovation Lab – Nordstrom
    Internal technology lab focused on innvoation around technology
    Secondary focus – but still in scope: operations, products, business models and management
    Goal is to deliver data-driven products to inform business decisions internally, and to enhance customer experience externally
    Multi-disciplinary team of techies, designers, entrepreneurs, statisticians, researchers and artists
GFDRR Innovation Lab – World Bank
    Global facility for disaster reduction and recovery, a global partnership, managed by the World Bank and funded by 25 donor partners
    supports use of science, technology, open data and innovation to empower decision-makers to increase their resilience
    tries to apply the concepts of the global open data movement to the challenges of reducing vulnerability to natural hazards and  the impacts of climate change through OpenDRI (Open Data for Resilience Initiative)
Big Data Innovation Center and Innovation Lab – SAP
    Focus on SAP’s mobile and cloud portfolio
    Mission is to extend SAP stack and develop innovative data-driven process applications leveraging an integrated platform and next-generation DB technologies
    Partnership and exchanges with leading schools including Stanford, MIT, Berlin universities
    Short, fast-paced innovation cycles
    Project run-times of a few months on an average
    Hands over prototypes to SAP development for turning into market-ready products
</code></pre>

<h3>翻訳</h3>

<p>未来をデザインするデータイノベーションのラボ</p>

<p>ビッグデータ革命の真っ最中、そして差し迫ったIOT革命、データを中心にしたイノベーションにあらたな熱狂が生まれている
。Googleが最初に打ち出したコンセプトのような複数のラボが、より多くの組織、ビジネスコミュニティ、政府や国によって始められ、データと分析の技術のイノベーションを生むためのラボをスタートアップしている。
これらのデータイノベーションラボの背景にあるのは、アヴァンギャルドデータ、分析技術、Agile fashionの中での商品とコンセプトから製品化までの迅速な技法を開発するという点にある。
巨大な組織や政府内部で伝統的な官僚性の元、これらのラボはイノベーションの文化を育てるよりよい機会を伺い、彼らの自主的で自治的なあり方から、彼らのスタートアップの「独特なやり方」が形作る文化は、アジャイル方法論に影響を与えている。</p>

<p>データと分析の分野で親組織のために利益を得る目的で設置された、いくつかのデータイノベーションラボを下記にリストしている。ビッグデータとデータサイエンスの分野でデータ商品の作成を試作している。</p>

<pre><code>Data Innovation Lab – Thomson Reuters
    １０人ほどの少ないグループ、内部のチーム、サードパーティー、顧客とデータ運用のイノベーションを見つけるためにパートナとなっている。
    刷新された方法での内部データと外部データのマッシュアップによる実験
    内部のクラウドソースコンペティションをホストしている。
    ビジネスプロブレムを技術データ問題の文章に翻訳している。
    Exchangeを作成：考えや洞察をきょうゆうするためのデジタルフォーラム
    新しいデータイノベーションの機会のためにマーケットサイズと潜在性の見積もりをするための中央戦略とパートナ－である。
The Data Lab – Scotland
    ミッションはスコットランドの地元産業を強くすることである。また、世界市場の情報科学とコンピュータサイエンスにおいて世界をリードする研究へと移行することだ。
    適切なスキルで武装された才能あふれるデータサイエンティストたちのパイプラインを作成するために産業と働くことによりトレーニングとスキルに焦点をあてている。
    世界を牽引する研究者たちと地元の産業と公共部門の組織を通し、データサイエンティストと結び付け、主要な問題を解決するための協力を要請できるようなエキスパートにアクセスさせている。
Smart Data Innovation Lab – German government
    カールスルーエ工科大学（KIT）によってホストされている。そのミッションはビッグデータをスマートデータに落とし込むことだ。
    研究目的のために非常にセキュアな環境に、データを中央に貯蔵する計画をしている。
    SAP HANAやIBM Watoson, ハードウェアはIBM Power, Intel architecturesなど、ビッグデータをプロセスするための最新のインフラを揃えている。
    クラウドソース、オープンソースのデータで補完された実行環境から直接、データソースを出力する産業パートナー
    調査内で再利用するためにオープンソースレポジトリを提供する計画を立てている。
Midata Innovation Lab – UK government
    UKのBIS(ビジネスイノベーションと産業関連能力部)によって経営されている組織
    ビッグデータから、消費者のための新しいサービスを作成するビジネスを後押しする。
    個人クラウド、もしくは個人データストアのコンセプトを含み、動いている。
    PaogaとMydex、Allfieldという三つのＰＤＳのリードで働いている。
    参加している組織や開発者はPDSを使用して、新しい革新的なサービスを消費者に提供します。
Nordstrom Innovation Lab – Nordstrom
    内部の技術ラボは技術の周辺のイノベーションに焦点を当てている。
    第二の焦点ーしかしスコープに入っている点：運用、製品化、ビジネスモデルとマネジメント
    ゴールは内的にビジネスの決定を行うためのデータ運用製品をデリバリすること
    技術者と、デザイナー、起業家、統計学者、研究者とアーティストなどの複数分野にわたるチーム
GFDRR Innovation Lab – World Bank
    災害削減と復旧のための国際組織で、25の寄付パートナーにファンドされ世界銀行によってマネージされているグローバルパートナーシップ。
    科学と技術、オープンデータとイノベーションを利用し、ディシジョンメーカーが彼らの復興を増加させるために力添えしている。
    OpenDRI (Open Data for Resilience Initiative)を通して自然災害と気候変動のインパクトへの脆弱性を減じる挑戦へと世界のオープンデータの動きのコンセプトを適用させようとしている。
Big Data Innovation Center and Innovation Lab – SAP
    SAPのモバイルとクラウドのポートフォリオにフォーカス
    ミッションはSAPのスタックを拡張することと、次の世代のDB技術と統合プラットフォームをテコ入れするイノベイティブなデータ運用プロセスアプリの開発。
    スタンフォード、ベルリン、ミットを含んだ牽引学校とパートナー、交換を行っている。
    短く、速いイノベーションサイクル
    平均で数か月経営のプロジェクト
    市場に出す準備の製品のプロトタイプをSAPの開発者に引き渡している。
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Gentle Introduction to Machine Learning]]></title>
    <link href="http://narusou.github.io/blog/2017/07/24/a-gentle-introduction-to-machine-learning/"/>
    <updated>2017-07-24T13:42:30+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/24/a-gentle-introduction-to-machine-learning</id>
    <content type="html"><![CDATA[<!--more-->


<h3>原文</h3>

<p>A gentle introduction to Machine Learning
Posted on February 5, 2015 | Leave a comment
Machine Learning is a big part of big data and data science. A subset of artificial intelligence – a branch of science notorious for requiring advanced knowledge of mathematics. In practice though, most data scientists don’t try to build a Chappie  and there are simpler, practical ways to get started with machine learning.
Gmail Priority Inbox
Gmail Priority Inbox
Machine learning in practice involves predictions based on data. Notable examples include Amazon’s product recommendations with the “customers also bought” scroll-list, or Gmail’s priority inbox or any email spam-filter feature. How do these work? For Amazon, clicks by the user is used to learn and predict user behavior and propensity (likelihood) to buy certain items. The items the user is most likely to buy are then displayed on the recommendation system. Gmail’s system learns from the messages which the user reads and/replies to and prioritizes them.
In both cases, there are some predictions made based on certain example usage of the data. Thus in essence, machine learning is about predictions based on models, which themselves are created based on examples.
More specifically, a machine learning model is a set of explanations of the relationships between the input data and the output predictions. These relationships are discovered from examples of input-output pairs. In machine learning terminology, the input data is also called features and the predictions are called output. Once a model has been created, it can be used on new inputs to predict outputs.
Machine learning models therefore “learn” to predict from examples. This learning is also known as “training” the model, and the associated good quality data-set is called “training data-set“. The stage where the model is used on new inputs is known as “testing” the model, and the data-set associated with it is called “test data-set“.
There are different ways to perform this learning, with different types of algorithms to build models and perform predictions. Most common among these are Classification and Regression techniques. The Gmail spam-filter is an example of Classification technique. Given a set of emails marked as spam or not-spam, it learns the characteristics of emails and is then able to process future email messages to mark them as spam or not. Classification deals with prediction of which class ordinal data fits in, while regression deals with prediction of continuous numeric data. Example of regression is a best-fit line drawn through some data points for generalization. Both classification and regression are examples of supervised learning, as the algorithm is told to predict a label or target value.
The opposite of this is unsupervised learning – where there’s no label or target value given for the data. An example of this could be clustering – a task of grouping a items so that objects in the same group or cluster are more similar (in some sense or another) to each other than to those in other groups (clusters).
Machine Learning Techniques
Machine Learning Techniques
With so many choices, how do you choose the right algorithm? Without considering nuances of the data, a rule of thumb is to look at the objective of the prediction:</p>

<pre><code>If the prediction is to forecast a target value, we use supervised learning, else use unsupervised learning or density estimation algorithms.
</code></pre>

<p>It is important to note though, that this is not unbreakable, rather usage of algorithms is rather fuzzy. This is quite common in machine learning, where most problems are not deterministic in nature, and often a bunch of different algorithms are tried out to see how they perform. There are also ensemble models like Gradient Boosting – a regression technique which  uses an ensemble of weak prediction models, typically decision trees to get an improved prediction model. An interesting tool based on symbolic regression, which infers the model from the data is Nutonian Eureqa, also dubbed as the robotic data scientist.
Many algorithms are different, but the steps to use one are similar:
Collect data > Prepare data > Analyze input data > Clean data/verify data quality > Train the algorithm > Test the algorithm > Iterate/Deploy. (See also my earlier post on the data science project lifecycle)
As with many other aspects (data wrangling) of data science, both R and Python are very popular languages for using machine learning techniques. There are also start-ups like BigML providing MLaaS or Machine Learning as a Service.
In conclusion of this post, a few points to remember: garbage in – garbage out:- data quality matters as much if not more than algorithms, quantity of data or complexity of algorithm are not substitutes for quality, and of course as with all predictions, machine learning can be wrong as well.</p>

<h3>翻訳</h3>

<p>機械学習への優しい導入</p>

<p>ビッグデータの大部分である機械学習は、人工知能の一部で、高度の数学知識を要求する悪名高い科学の一部門として知られている。実践では、ほとんどのデータサイエンティストはChappieを作成しようとはしない。機械学習を始めるのにはより簡単で実践的な方法がある。
実践の機械学習はデータをベースにした予測を伴っている。アマゾンの「customer also bought」の商品推薦スクロール、Gmailの優先順位、スパムフィルターの特性を持つEmail等が目立った例として挙げられる。これらはどうやって動いているのか。
アマゾンの場合にはユーザーによるクリックによって学習され、そのユーザーの動きや、あるアイテムを買う傾向を予知している。このユーザーに買われそうなアイテムは、そうして推薦システム上にディスプレイされる。Gmailのシステムはユーザーが「読んだ」、「返事したメッセージ」から情報を学び、それらを優先順位付けする。</p>

<p>両方とも、データが例示された確実に使用されたものを基礎に予測がなされている。それゆえ主要な点として、予想に関する機械学習は、それらの例を基礎として作成されたモデルを基にしている。さらに具体的には、機械学習モデルはインプットされたデータとアウトプットされた予測の間の関係に対する纏まった説明であるともいえる。</p>

<p>これらはインプットとアウトプットのペアの具体例から発見されている。機械学習の技術では、インプットはfeatures[特徴]とよばれ、アウトプットはpredictions[予想]と呼ばれている。一度モデルが作られると、新しいインプットからアウトプットを予想するために使用される。機械学習モデルは例から予想するために「学習」する。この学習はまた、モデルの「訓練」として知られており、この関連したよい品質のデータセットは「トレーニングデータセット」として呼ばれている。
新しいインプットの上でモデルが使用される段階はモデルを「テストする」として知られており、これらに関連するデータセットは「テストデータセット」として知られている。</p>

<p>この学習を実行するための異なる方法がいくつかある。これらの中でもっともよく知られているのは、「分類」と「回帰」の技術だ。Gmail Spam Filterは「分類」の技術の一例になる。スパムかそうでないかといったマークがつけられたEmailのセットが与えられると、そのEmailの特徴を学び、未来のemailメッセージをスパムかそうでないかマークするためのプロセスが可能になる。「分類」はどの分類普遍データがふさわしいかの予想を取り扱い、「回帰」は継続した数的なデータの予想を取り扱う。回帰の例は一般化のためのいくつかのデータポイントを通して導き出された最も適切な水準だ。分類も回帰も教師信号ありの学習の例で、目的の価値、もしくはラベルを予想するためのアルゴリズムとして示されている。</p>

<p>この反対が、教師信号なしの学習であり、そのデータのために与えられるような目的の値やラベルといったものはない。この例としてはクラスタリングが相当しうる。クラスタリングは項目をグルーピングするタスクで、同じグループ、もしくは他のクラスタの中の対象物はお互い、他のクラスタの内部の対象よりも（どこか、もしくは複数個所が）似ている。</p>

<p>なぜこんなにも複数の選択があるのか？
どうやってそのアルゴリズムを学ぶのか？
データのニュアンスを考慮せずに、予想の目的に注目することが親指ルール（大まかなやり方）だ。</p>

<p>・もしも対象の値を予想することが今回のpredictionであった場合には、私たちは教師あり信号を使用する。そうでなければ教師なし信号、密度見積もりアルゴリズムを使用する。</p>

<p>それは絶対のルールではなく、むしろアルゴリズムの使用方法はかなりファジーだと気づくのは重要だ。これは機械学習ではかなり普通のことで、自然界ではほとんどの問題は決定論的ではなく、それらがどういうものかを確認するために多くの違ったアルゴリズムが試される。
またGradient Boostingのようなアンサンブルモデルもあり、弱い予見モデルのアンサンブルを使用した回帰技術を使用している。典型では、改善された予想モデルを取り入れるための意思決定ツリーのようなものがある。
記号回帰を基礎としたデータからモデルを推量するNutonian Euraqaという興味深いツールも存在している。これはロボティックデータサイエンティストとも呼ばれている。</p>

<p>多くのアルゴリズムがお互い異なり、使うステップは一辺倒で似ている。</p>

<p>１、データ集め
２、データを準備
３、入力データを分析
４、データをクリア/データの品質を証明
５、アルゴリズムを訓練
６、アルゴリズムをテスト
７、繰り返し、Deploy</p>

<p>データサイエンスの多くの他の側面（データラングリング）により、RとPythonは両方とも機械学習を利用するために人気のある言語だ。BigMLのようなMLaasやサービスとして機械学習を提供するスタートアップもある。</p>

<p>この投稿の結論として、ゴミ入れ、ゴミ出し、そしてアルゴリズムの問題でなければ、データ品質の問題が多い。データ量やアルゴリズムの複雑さは品質の代わりには成り得ない。そしてもちろん、全ての予想、機械学習は間違いうるということだ。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why Spark Is the Big Data Platform of the Future]]></title>
    <link href="http://narusou.github.io/blog/2017/07/24/why-spark-is-the-big-data-platform-of-the-future/"/>
    <updated>2017-07-24T13:39:40+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/24/why-spark-is-the-big-data-platform-of-the-future</id>
    <content type="html"><![CDATA[<!--more-->


<h3>原文</h3>

<p>Why Spark is the big data platform of the future
Posted on March 23, 2015 | 1 Comment
Apache Spark has created a lot of buzz recently. In fact, beyond the buzz, Apache Spark has seen phenomenal adoption and has been marked out as the successor to Hadoop MapReduce.</p>

<p>Google Trends confirms the hockey stick like growth in interest in Apache Spark.  All leading Hadoop vendors, including Cloudera, now include Apache Spark in their Hadoop distribution.</p>

<p>So what exactly is Spark, and why has it generated such enthusiasm? Apache Spark is an open-source big data processing framework designed for speed and ease of use.  Spark is well-known for its in-memory performance, but that has also given rise to misconceptions about its on-disk abilities. Spark is in fact a general execution engine – which has a greatly improved performance both in-memory as well as on-disk, when compared with older frameworks like MapReduce. With its advanced DAG (directed acyclic graph) execution engine, Spark can run programs up to 100x faster than MapReduce in memory, or 10x faster on-disk. Why is Spark faster than MapReduce?</p>

<pre><code>A key step during MapReduce operations is the synchronization or  “shuffle” step, intermediate between the “map”-step and the “reduce”-step. Apache Spark implements a sort-based shuffle design, which improves performance.
Apache Spark also includes a DAG (directed-acyclic graph) which allow developers to execute DAGs all at once, not step by step. This eliminates the costly synchronization required by MapReduce. Note that DAGs are also used by Storm and Tez
Spark supports in-memory data sharing across DAGs, so different jobs can work with the same data at a very high speed.
</code></pre>

<p>It’s important to remember that Hadoop is a decade-old technology, developed at a time when memory was still relatively expensive, and therefore took the design approach of persistence to disk as a way of maintaining state between execution steps. On the other hand, Spark was developed at UC Berkeley AMPLab in 2009 and then it was open-sourced in 2010 – when memory had become much cheaper. Therefore, Spark stores data in memory and transparently persists it to disk if needed, thereby achieving better performance. The core concept of Spark is this programming abstraction over data storage – called RDDs (Resilient Distributed Dataset). Under the hood, Spark automatically distributes the data contained in RDDs across the cluster and parallelizes the operations performed on them.</p>

<p>The end result is that, on an average – the lines of code required to develop a distributed data processing program is much less in Spark, when compared with MapReduce. See more details on why Spark is the fastest open-source engine for sorting a petabyte. Clearly, faster execution has been one of the key reasons for the uptake of Spark, but Spark also provides further advantages. Similar to YARN, the upgrade of the Hadoop framework over the MapReduce-only version, Spark allows a wide range of workloads from batch to interactive and streaming. It reduces the burden of maintaining separate tools as in Hadoop – and provides APIs in Scala, Java, Python and SQL.  Spark can run over a variety of cluster-managers, including Hadoop YARN, Apache Mesos, and Spark’s own standalone scheduler. Spark components</p>

<p>Spark Core – provides basic functionality of Apache Spark, including RDDs and APIs to manipulate them. Spark SQL – A new component which replaces the older Shark (SQL on Spark) project, this package provides better integration with Spark Core, it allows querying data through SQL and HiveQL and supports many data sources from Hive tables, Parquet and JSON. Spark SQL also allows developers to intermix SQL queries with the code for data manipulations with RDDs in Python, Java, and Scala. It also provides fast SQL connectivity to BI tools like Tableau or QlikView.</p>

<p>Spark Streaming – based on micro-batching, this component enables processing of real-time streaming data. It uses DStreams, which are series of RDDs, to process real-time data. The Spark Streaming API is very similar to the Spark Core RDD APIs, making it easy for developers to reuse and adapt code for batch to interactive or real-time applications. MLlib – provides a library of machine learning algorithms including classification, regression, clustering, and collaborative filtering, as well as model evaluation and data import. GraphX – provides an API for graphs and graph-parallel computations and operators for manipulating graphs and a library of graph algorithms. The SparkR project aims to provide a light-weight front-end to use Apache Spark from R. Work is on to integrate SparkR into Spark. Recently, Spark has introduced a dataframe library with R/Pandas syntax for use across all of the Spark language APIs and an ML pipeline API which also integrates with data frames. Spark adoption is increasing manifold, boosted by increased third-part vendor support. Databricks – the company spun out of AMPLab by the creators of Apache Spark, now provides Spark as a service on the cloud – with its own Databricks Cloud – which is in private beta. The Databricks cloud is designed to support data science in the lab as well as in the factory – by creating polyglot notebooks (mix of Scala/Java/Python/SQL possible) and building production pipelines for ETL and analytics jobs. Tableau and MemSQL have provided Spark connectors, Altiscale now provides Spark in the cloud and machine learning vendors like Nube are building products like Reifier to perform entity resolution and de-duplication using Spark. ClearStory Data provides Spark-based data processing and analytics. There is also a fledgling community of packages for Apache Spark. Big data and data science projects are complex with an increasing diverse toolset which require massive integration efforts. Greater flexibility than that provided by MapReduce, capability to support a variety of workloads and a simpler, more unified ecosystem of tools which work out of the box on a general execution engine (Apache Spark) thus provide better simplicity than the complex zoo of Hadoop MapReduce projects. Together with SparkSQL and dataframes library, Spark democratizes access to distributed data processing beyond MapReduce programmers extending it to other developers and business analysts. Over and above, considering the fast performance of Spark, it is no wonder that Apache Spark continues to gain traction and looks all set to be the default framework for Big data processing in the near future. More info:</p>

<pre><code>Spark record for fastest sort of a petabyte 
Dataframes in SparkS
</code></pre>

<h3>翻訳</h3>

<p>なぜSparkが未来のビッグデータプラットフォームなのか</p>

<p>Apache Sparkが最近話題に上がってくることが多くなった。事実、そういった話題以上に、Apache SparkはHadoop MapReduceの後任として位置を確立し、また現象的な採用実績を誇っている。</p>

<p>グーグルトレンドはApache Sparkへの関心がホッケースティック成長であることを確認している。Clouderaを含めて全てのHadoopを使うリードベンダーが今、やApache Sparkを彼らのHadoop分散技術に組み込んでいる。</p>

<p>では、Apache Sparkとは一体何か。そして、なぜそんな熱心にもてはやされるのか？Apache Sparkはオープンソースのビッグデータ処理フレームワークで、使いやすさとスピードに特化してデザインされた。
Apache Sparkはそのin-memoryによるパフォーマンスでよく知られ、しかしそのon-diskでの有能さについて思い違いも生じている。スパークは事実としてはMapReduceのような古いフレームワークと比べられた時には、on-diskでもin-memoryでもとても改善されたパフォーマンスを発揮する総合実行エンジンだ。
その高度な有効無閉路グラフ実行エンジンにより、Apache Sparkはin memoryで100x、on deskで10xのスピード以上でプログラムを走らせることが可能になっている。では、なぜApache SparkはMapReduceよりも速いのか。</p>

<p>・MapReduceの場合、MapとReduceのステップの中間に、シャッフルステップ、もしくは同期の運用がキーステップとして存在している。Apache Sparkはパフォーマンスを改善するためにSort-based shuffleのデザインを実装している。</p>

<p>・Apache Sparkは開発者に一度にDAGsを実行させることができるDAG (directed-acyclic graph)もまた実装している。これはMapReduceによって要求されるコストがかかる同期を取り除いてくれるし、StormやTazでも使用されている。</p>

<p>・Apache SparkはDAGsを介してのin-memoryデータシェアリングをサポートするので、同時にハイスピードで同じデータ上で違うジョブの稼働が可能になっている。</p>

<p>Hadoopはメモリが依然として相対的に高価であったような10年前に開発された産物であることを思い出すことは重要であるし、それゆえ実行するステップの合間に保守状態の方法としてディスクに固執したアプローチのデザインを取っていた。
一方、Sparkは2009年にバークレー大学でAMPラボで開発され、メモリがかなり安くなった2010年にOpenSourceになった。それゆえ、Sparkはメモリにデータを貯蔵し、必要であればディスクへ見える状態で入れておくこともする。これにより、よりよいパフォーマンスを達成している。</p>

<p>SparkのCoreコンセプトはRDDs (Resilient Distributed Dataset)と呼ばれるデータストレージを覆っている、プログラミング抽象化にある。この屋根の元で、Sparkはそれらの上で行われる平行した運用とクラスタを横断したRDDs (Resilient Distributed Dataset)の内部に含まれる処理を自動的に分散する。</p>

<p>【Work-count code in Spark&rsquo;s Python API】
file = spark.textfile(&ldquo;hdfs://&hellip;&rdquo;)
file.flatfile(lamba line: line.split())
.map(lamba word: (word, 1))
.reduceByKey(lambda a, b: a+b)</p>

<p>分散型のデータプロセスを行うプログラムを開発するのに要求されるコードの行数はMapReduceと比較した時にSparkでは非常に平均として、結果としては、少ない行数となる。</p>

<p>もっと詳しい説明に関しては下記を参照。
<a href="https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html">https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html</a></p>

<p>明らかなSparkを採用する主要な理由の一つとしては、その速さが挙げられるが、Sparkはさらなる利点をもっている。
YARN、HadoopフレームワークのupgradeはMapReduceのみのバージョンと似ていて、Sparkはバッチを起点とし、対話とストリーミングへと、広い幅の作業を許容している。
これはHadoopのようにバラバラのツールを保守するための負荷を減じ、SQLやPython,Java,ScalaのAPIを与えてくれる。SparkはApache Mesos、Haddop YARN、そしてSpark自身のスタンドアローンスケジューラ－を含む様々なクラスタマネージャーをくるみ稼働する。</p>

<p>Spark Core- Apache SparkのRDDsと、それを操作するためのAPIを含む基本的な機能
Spark SQL- 古いShark(SQL on Shark)のプロジェクトにとって代わる新しいコンポネント。このパッケージはShark Coreとのより良い統合を提供するパッケージで、ParquetやJSON、Hivのテーブルからの多くのデータソースをサポートし、SQLとHiveQLをとおしてデータをクエリすることを許容する。
Spark SQLは開発者にPythonとJava、Scalaの内部でRDDsによるデータ操作のためのコードとSQLのクエリを混ぜることもさせる。これはまた、TableauやQlick ViewなどのBItoolへのfastなSQL接続を提供する。
Spark Streaming- マイクロバッチングをベースにして、このコンポネントはリアルタイムデータストリーミングのプロセスを利用可能としている。これはリアルタイムデータプロセスをするために、RDDsのシリーズであるDStreamsを使用している。
Spark Streaming APIは開発者にリアルタイムアプリ、もしくは対話形式のアプリへのバッチのコードを再利用、改変を簡単にするSpark Core RDD APIsにとても似ている。
MLib- 回帰、クラスタリング、分類、そして協調フィルタリング、データインポートとモデル評価をも含む機械学習のためのアルゴリズムライブラリを供給する。
GraphX- グラフアルゴリズムのライブラリとグラフを操作するためのグラフ平行計算と演算子、グラフ自身のためのAPIを提供する。
このSparkRプロジェクトはRからApache Sparkを使用するための軽いフロントエンドを提供することを目途としている。最近、SparkはR/Pandasのシンタクスでデータフレームライブラリを導入していて、R/Pandasのシンタクスは全てのSpark言語 APIと ML パイプラインAPIを通して使用でき、これらもまたデータフレームによって統合されている。Sparkの採用は多方面に広がっており、サードパーティーのベンダーサポートの増加によって加速している。Apache Sparkの製作者達によるAMPLabからのスピンアウト会社、Databricksは現在、Closedのベータ版であるDatabricks自身のクラウド上でクラウドサービスとしてSparkを提供している。Databricksクラウドは多言語ノートブックス（可能なScala,Java,Python,SQLの混合）の制作と、ETLと分析の仕事のための製品経路をたてることによって、工場やラボでのデータサイエンスをサポートするためにデザインされた。TableauとMemAQLはSparkコネクタを提供し、Altiscaleは現在クラウドでSparkを提供している。そしてNubeのような機械学習のベンダーはSparkを使用してentity resolutionと重複削除を行うためのReifierのような製品を打ち立てている。Clear Story DataはSparkを基礎としたデータプロセスと分析を提供している。ここにはまた生まれたてのApache Sparkのためのパッケージのコミュニティーもまたある。ビッグデータとデータサイエンスのプロジェクトは巨大な統合のための努力が必要な多様化したツールセットの増加により、複雑だ。MapReduceによって供給されるもより素晴らしい柔軟性、作業と簡単さのバラエティをサポートする許容性、一般実行エンジン上のボックスを動かす、より一元化されたツールの環境システム。それゆえ、Hadoop MapReduceプロジェクトの複雑な動物園よりも、よりよい単純さというものを提供している。SparkSQLとデータフレームライブラリとともに、MapReduceのプログラマーだけでなく、ビジネスアナリストや他のプログラマーたちにも分散データプロセスへのアクセスへの参政権をSparkは 与えている。以上より、Sparkの速いパフォーマンスを考慮すると、近い将来にビッグデータプロセスのための全てのデフォルトフレームワークに、そして牽引し続けていっても不思議ではない。</p>

<p>詳細情報
<a href="https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html">https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html</a>
<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes">https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to Data Science]]></title>
    <link href="http://narusou.github.io/blog/2017/07/24/An%20introduction%20to%20Data%20Science/"/>
    <updated>2017-07-24T13:36:54+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/24/An introduction to Data Science</id>
    <content type="html"><![CDATA[<!--more-->


<p>I presented a talk last week introducing Data Science and associated topics to some enthusiasts.
Here’s a slide deck I created quickly with markdown using Swipe – a start-up building HTML5 presentation tools.
Here are the slides: <a href="https://www.swipe.to/2675ch">https://www.swipe.to/2675ch</a></p>

<p>データサイエンスへの導入
私は先週データサイエンスの導入と関連したトピックに関して熱心な人たちにプレゼンをした。
マークダウン記述でスワイプを使って即興で作成したスライドデッキをここに置いておく。スワイプはHtml5のプレゼンテーションツールを作成したスタートアップだ。
Here are the slides: <a href="https://www.swipe.to/2675ch">https://www.swipe.to/2675ch</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Science Project Life-cycle]]></title>
    <link href="http://narusou.github.io/blog/2017/07/21/data-science-project-life-cycle/"/>
    <updated>2017-07-21T14:49:44+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/21/data-science-project-life-cycle</id>
    <content type="html"><![CDATA[<!--more-->


<h1>原文</h1>

<h3>The data science project lifecycle</h3>

<p>How does the typical data science project life-cycle look like?
This post looks at practical aspects of implementing data science projects. It also assumes a certain level of maturity in big data (more on big data maturity models in the next post) and data science management within the organization. Therefore the life cycle presented here differs, sometimes significantly from purist definitions of ‘science’ which emphasize the hypothesis-testing approach. In practice, the typical data science project life-cycle resembles more of an engineering view imposed due to constraints of resources (budget, data and skills availability) and time-to-market considerations.
The CRISP-DM model (CRoss Industry Standard Process for Data Mining) has traditionally defined six steps in the data mining life-cycle. Data science is similar to data mining in several aspects, hence there’s some similarity with these steps.</p>

<p>The CRISP model steps are:
1. Business Understanding
2. Data Understanding
3. Data Preparation
4. Modeling
5. Evaluation and
6. Deployment</p>

<p>Given a certain level of maturity in big data and data science expertise within the organization, it is reasonable to assume availability of a library of assets related to data science implementations. Key among these are:</p>

<ol>
<li>Library of business use-cases for big data/ data science applications</li>
<li>Data requirements – business use case mapping matrix</li>
<li>Minimum data quality requirements (test cases to ensure minimum level of data quality to ensure feasibility)</li>
</ol>


<p>In most organizations, data science is a fledgling discipline, hence data scientists (except those from actuarial background) are likely to have limited business domain expertise – therefore they need to be paired with business people and those with expertise in understanding the data. This helps data scientists gain or work together on steps 1 and 2 of the CRISM-DM model – i.e. business understanding and data understanding.The typical data science project then becomes an engineering exercise in terms of a defined framework of steps or phases and exit criteria, which allow making informed decisions on whether to continue projects based on pre-defined criteria, to optimize resource utilization and maximize benefits from the data science project. This also prevents the project from degrading into money-pits due to pursuing nonviable hypotheses and ideas.</p>

<p>The data science life-cycle thus looks somewhat like:</p>

<ol>
<li>Data acquisition</li>
<li>Data preparation</li>
<li>Hypothesis and modeling</li>
<li>Evaluation and Interpretation</li>
<li>Deployment</li>
<li>Operations</li>
<li>Optimization</li>
</ol>


<p>Data Acquisition – may involve acquiring data from both internal and external sources, including social media or web scraping. In a steady state, data extraction and transfer routines would be in place, and new sources, once identified would be acquired following the established processes.</p>

<p>Data preparation – Usually referred to as “data wrangling”, this step involves cleaning the data and reshaping it into a readily usable form for performing data science. This is similar to the traditional ETL steps in data warehousing in certain aspects, but involves more exploratory analysis and is primarily aimed at extracting features in usable formats.</p>

<p>Hypothesis and modeling are the traditional data mining steps – however in a data science project, these are not limited to statistical samples. Indeed the idea is to apply machine learning techniques to all data. A key sub-step is performed here for model selection. This involves the separation of a training set for training the candidate machine-learning models, and validation sets and test sets for comparing model performances and selecting the best performing model, gauging model accuracy and preventing over-fitting.</p>

<p>Steps 2 through 4 are repeated a number of times as needed; as the understanding of data and business becomes clearer and results from initial models and hypotheses are evaluated, further tweaks are performed. These may sometimes include Step 5 (deployment) and be performed in a pre-production or “limited” / “pilot” environment before the actual full-scale “production” deployment, or could include fast-tweaks after deployment, based on the continuous deployment model.</p>

<p>Once the model has been deployed in production, it is time for regular maintenance and operations. This operations phase could also follow a target DevOps model which gels well with the continuous deployment model, given the rapid time-to-market requirements in big data projects. Ideally, the deployment includes performance tests to measure model performance, and can trigger alerts when the model performance degrades beyond a certain acceptable threshold.
The optimization phase is the final step in the data science project life-cycle. This could be triggered by failing performance, or due to the need to add new data sources and retraining the model, or even to deploy improved versions of the model based on better algorithms.
Agile development processes, especially continuous delivery lends itself well to the data science project life-cycle. As mentioned before, with increasing maturity and well-defined project goals, pre-defined performance criteria can help evaluate feasibility of the data science project early enough in the life-cycle. This early comparison helps the data science team to change approaches, refine hypothesis and even discard the project if the business case is nonviable or the benefits from the predictive models are not worth the effort to build it.</p>

<h1>翻訳</h1>

<h3>データサイエンスプロジェクトライフサイクル</h3>

<p>典型的なデータサイエンスプロジェクトのライフサイクルはどのようなものか？
この投稿では実装のデータサイエンスプロジェクトの実践的な側面をみていく。次の投稿においてはビッグデータ習熟モデルについて投稿するが、本投稿では習熟モデルと組織内でのデータサイエンスマネジメントのある水準を仮定していく。
それゆえ、ここでプレゼンされるライフサイクルは、仮説検証を強調する科学の純粋な定義とは時として大幅に異なる。
実践の中では、この典型的なデータサイエンスプロジェクトのライフサイクルは製品化までの完了期限への考慮、予算やデータと能力可用性のリソースに制限されたエンジニアの視点に共通項を持ち合わせる。</p>

<p>このCRISM-DMモデル（CRoss Industry Standard Process for Data Mining）はデータマイニングのライフサイクルの中で伝統的に６つのステップに定義されている。データサイエンスは何点かの側面からデータマイニングに似ている、それゆえそこにはこれらのステップにおいてもいくつかの類似点がある。</p>

<p>【CRISM-DMモデルのステップ】<br/>
１、ビジネスへの理解<br/>
２、データへの理解<br/>
３、データの準備段階<br/>
４、モデリング<br/>
５、評価<br/>
６、ディプロイ</p>

<p>組織でのデータサイエンスの専門知識とビッグデータの習熟度の水準さえあれば、データサイエンスの実装に関係したアセットライブラリの可用性を仮定することはリーズナブルだといえる。</p>

<p>【Key points】<br/>
１、ビッグデータとデータサイエンスのアプリケーションに関してのビジネスユースケースのライブラリ<br/>
２、データの要求-ビジネスユースケースのマッピングマトリクス<br/>
３、最低限のデータ品質の要求（実行可能性を保障するデータ品質の最低限の水準を保証するテストケース）</p>

<p>ほとんどの組織において、データサイエンスは生まれたての分野であり、結果としてデータサイエンティストたちは（数理のバックグラウンドを持っている者たちをのぞき）ビジネスについては専門知識が制限されているし、それゆえ彼らはビジネス側の人と組み、データに精通した専門知識とビジネス知識を一緒にする必要がある。
これはデータサイエンティストがCRISM－DMモデルのビジネス理解、データ理解といった１、２のステップに進むのを手助けし、協働することを手助けする。</p>

<p>事前に定義された基準を基礎としてプロジェクトをつづけるかどうかの意思決定をさせるような、リソース利用の最適化とデータサイエンスプロジェクトの有益性を最大化するような完了標準と、フェーズやステップの定義されたフレームワークに関して、この典型的なデータサイエンスのプロジェクトはエンジニアリングの実践演習になる。
これはまたプロジェクトがあり得ない仮説と考えを追いかける金食い虫に成り下がることを防止する。
このデータサイエンスのライフサイクルはそれゆえ、いくらか下記のようになる。</p>

<p>１、データ獲得<br/>
２、データ準備<br/>
３、仮説とモデリング<br/>
４、評価と解釈<br/>
５、ディプロイ<br/>
６、運用<br/>
７、最適化</p>

<p>データ獲得-ウェブスクレイピングやソーシャルメディアを含む内部と外部のデータソース、両方からのデータ獲得を意味している。安定した状態では、データのエクストラクションと移動のルーティンが実行され、新しいソースが確認され次第、打ち立てられたプロセスに従って取得される。</p>

<p>データ準備-データラングリングとして参照され、データのクリーニングとデータサイエンスのパフォーマンスのために容易に利用できる形に落とし込むステップを含んでいる。</p>

<p>これはデータウェアハウスの側面における伝統的なETLのステップに似ているが、より予備的な分析と主に利用可能なフォーマットの中ではエクストラクトの特徴を狙いとしている。</p>

<p>仮説とモデリング-これは伝統的なデータマイニングのステップであり、しかしながらデータサイエンスのプロジェクトではこれらは統計的なサンプルデータに限定されていない。実際には、この考えは機械学習の技術をすべてのデータに適用するためにある。カギになるサブステップがここでモデルの選択のためになされる。<br/>
これは機械学習のモデル候補のトレーニングセットの分割を含み、また最高のパフォーマンスを出すモデルパフォーマンスの選択とモデルの正確さを図り、過学習を防ぐ。</p>

<p>ステップ２から４は必要に応じて何回も繰り返される、というのもビジネスとデータの理解がよりクリアになり、最初のモデルと仮説から評価され、さらに微調整も行われるためだ。継続移設モデル（continuous deployment model）をもとに、これらは時折、実際のフルスケールの本番環境にdeploymentされる前に、限定された、もしくはパイロット環境、プレ本番環境でステップ5（deployment）を行うことも含み、deploymentのあとの即時調整も含みうる。</p>

<p>一度このモデルが製品にdeploymentされると、定常業務と保守の時間がくる。この運用の段階もまた、ビッグデータのプロジェクトでの急な製品化への期限要求がくれば、継続移設モデル（continuous deployment model）と相性のいい対象のDevOpsに従いうる。理想的には、このdeploymentはモデルパフォーマンスをはかるためのパフォーマンステストを含み、このモデルパフォーマンスが許容できる閾値を超えて程度が低い時にはアラートを鳴らすことが出来ることだ。</p>

<p>最適化の段階はデータサイエンスのライフサイクルにおいて最後の段階になる。この段階はパフォーマンスの落ち込みにより、もしくは新しいデータソースが追加されたためモデルの再トレーニングをしなければならないことにより、またはよりよいアルゴリズムを基にした改良されたより良いモデルのversionをdeployするために使われる。</p>

<p>アジャイルのdeploymentプロセス、特に継続的なデリバリはそれ自身の多くをデータサイエンスプロジェクトのライフサイクルに貸し出している。以前にお話ししたように、熟練され、良く定義されたプロジェクトのゴール、事前に定義されたパフォーマンスの標準はそのライフサイクルの中の十分早い段階で実行可能性を評価することを手助けすることができる。この比較はデータサイエンスチームがアプローチを変更したり、仮説を再精製したり、予見されるモデルからの利益が組み上げる価値のないものだったりビジネスケースが育成不能なものであれば、プロジェクトを廃棄することもでき、早い段階で助けになる。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Roles Do You Need in Your Data Science Team?]]></title>
    <link href="http://narusou.github.io/blog/2017/07/19/what-roles-do-you-need-in-your-data-science-team/"/>
    <updated>2017-07-19T13:10:55+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/19/what-roles-do-you-need-in-your-data-science-team</id>
    <content type="html"><![CDATA[<!--more-->


<h2>原文</h2>

<p>Over the past few weeks, we’ve had several conversations in our data lab regarding data engineering problems and day to day problems we face with unsupervised data scientists who find it difficult to deploy their code into production.</p>

<p>The opinions from business seemed to cluster around a tacit definition of data scientists as researchers, primarily from statistics or mathematics backgrounds, who are experienced in machine learning algorithms and often in some domain areas specific to our business, (e.g. actuaries in insurance), but not necessarily having skills of writing production-ready code.
The key driver behind the somewhat opposing strain of thought came from the developers and data engineers who often quoted Cloudera’s Director of Data Science – Josh Wills – famous for his</p>

<pre><code>“definition of a data scientist tweet”:
“Data Scientist (n.): Person who is better at statistics than any software engineer
 and better at software engineering than any statistician.”
</code></pre>

<p>Wills’ quote reflects the practical issues in finding “unicorn” data scientists and having to do with the best of what’s on offer for a multi-disciplinary area like data science. It’s also perhaps based on his work in startups like Cloudera and web giants like Google, where adopting agile practices like DevOps allow data scientists closer interaction with engineers and therfore substantial experience in deploying to production. Unfortunately, that’s always a challenge due to bureaucracy, mindset, lack of informed opinion and cultural barriers in larger or old-world organizations with legacy systems and practices.</p>

<p>As in any startup or lab working on problems in data science and big data, it’s important for us to clear misconceptions and get the team to a shared understanding of commonly used terms to establish a foundational common language, which would then allow developing a shared vision around our objectives. Therefore it’s necessary to review going beyond definitions of the “unicorn” data scientists and looking at what happens in real-life teams where data scientists work, like ours.</p>

<p>Different perspectives
A lot of the data scientists actually think of themselves as mathematicians, trying to formulate business problems into math/statistics problems and then trying to solve them in the data science projects.
However, the popular misconception arise sometimes out of the big-data hype articles churned out by big data vendors, including some evangelists – who equate data scientists with superpowers across a multitude of disciplines.
The developer’s views arise due to their unique perspectives on the complexities of data wrangling and fragmentation around tools, technologies and languages.</p>

<p>The reality, as always, is quite different from the hype. There are actually probably just a handful of the “unicorn” data scientists on the planet, who have superpowers in maths/stats,AI/machine learning, a variety of programming languages, an even wider variety of tools and techniques, and of course are great in understanding business problems and articulating complex models and maths in business-speak. For the lesser mortals, and less fortunate businesses, we have to do with multiple individuals to combine these skillsets together into a team or data science squad.</p>

<p>Building data science teams</p>

<p>In terms of hiring, building a data science team becomes much easier, once we get around the idea that the “unicorn” data scientists are not really available. The recruitment team and hiring manager can then focus on the individual skills that are required on the team and try to hire for profiles with strengths in these skills. Once hired, the manager’s role switches to building the team in terms of setting expectations and facilitating collaborative team dynamics to evolve self-governing teams, which can then focus on achieving the objectives in a collaborative manner, instead of having to be superheroes.
Dream data science team?</p>

<p>The roles in a data science team</p>

<p>So what roles would a data science team have? Depending upon the organizations’ objectives, the team could either focus on service-oriented consulting gigs, or focus on building reusable assets or data products.</p>

<pre><code>Data scientist – this role would be primarily of someone who can work on 
large　datasets (usually on Hadoop/Spark) with machine learning algorithms,
develop predictive models, understand the “theory” – maths and stats behind
the models and can interpret and explain model behavior in jargon-free language. 
Typically this role requires good knowledge of SQL and familiarity with at least
one programming language for predictive data analysis e.g. R and/ Python.
</code></pre>

<p>Netflix requirements for data scientist role</p>

<pre><code>Data engineer / Data software developer – this role is for someone who has good
understanding of distributed programming, including insfrastructure and architecture.
Typically this person is comfortable with installation of distributed programming
frameworks like Hadoop MapReduce/Spark clusters, can code in more than one programming
languages like Scala/Python/Java, and knows Unix scripting and SQL. Based on range and
depth of experience, this role can evolve into one of the two specialized roles 
– that of the data solutions architect and the data platform administrator.
</code></pre>

<p>Netflix requirements for data engineer role</p>

<pre><code>Data solutions architect – A data engineer with a range of deep experience
across several distributed technologies, and who also has good understanding 
of service-oriented architecture concepts and web applications 
(SOA concepts and REST frameworks) in addition to the developer skillsets.

Data platform administrator – A data engineer who has extensive experience 
across distributed technologies, especially managing clusters including production
envionments and good knowledge of cloud computing architectures
(public clouds like AWS if using public cloud or OpenStack and Linux sysadmin 
experience if using private/hybrid clouds)

Full-stack developer – This is an optional role – only required for teams 
which are focused on building data products with web interfaces. 
The full-stack developer is ideally an experienced web developer with experience 
in both backend and front-end e.g. a MEAN developer with experience on MongoDB,
Express, AngularJS and NodeJS.

Designer – this role demands an expert who has deep knowledge of user experience (UX) 
and interface design, primarily for web/mobile applications depending on target form
factors of the data product as well as data visualization and desirably 
some UI coding expertise. Building quick mockups and wireframes design is often 
required during product definition, and the designer needs to be able to work with
business as well as developers in a collaborative fashion. 
Sometimes this role is played by front-end UI developers as good designers 
don’t come cheap.

Product manager – This is an optional role – only required (but the key one) 
for teams focused on building data products. Defining the product vision, 
translating business problems into user stories, and focusing on getting 
the development team to build data products based on the user stories, 
aligning product releases and overall roadmap to business requirements 
and expectations is a key requirement from this role. Having product 
management experience along with relevant technical expertise is critical 
for this role due to differences in life-cycles of products and IT projects, 
as also the ability to present the voice of the customer and balance 
long-term vision with short-term needs. Back-filling this role with 
data scientists/developers who do not have product vision/business acumen 
is dangerous due to lures of gold-plating and lack of project management skills.

Project manager role may also be optionally required when the team is 
low on experience. In most successful cases of performance, managers set 
the objectives and expectations and facilitate to build self-governing teams
following agile practices.
</code></pre>

<p>Irrespective of whether the data science teams focus on consulting services in one-off projects or build data products which are reused, in both cases, the team would still require a minimum foundation to build on – in terms of processes or shared understanding, and tools and platforms to perform the actual work. We’ll review the data engineering requirements for such tools and platforms in the next post.</p>

<h2>翻訳</h2>

<h3>データサイエンスチームにはどんな役割が必要か？</h3>

<p>数週間前に私達のデータラボで、管理されていないデータサイエンティストがproductionにコードをdeployするのに困難を極めているといった、日々の問題やエンジニアリングの問題に関して何度となく話し合いが行われた。
統計や数学と言ったリサーチ畑のバックグランドから、ビジネス側の意見はデータサイエンティストの「暗黙の了解」の周りに集まったようだ。というのも、彼らは機械学習のアルゴリズムを経験していたり、我々の分野に特定されるような（保険の数理等）、しかしコードを読むスキルセットが必要としない業務経験があった。</p>

<p>こういった歪んだ考えに向かっていくようなKey DriverはClouderaのデータサイエンスのディレクター（Jhon Wills）をよく引用する。(データサイエンティストの定義のツイートで有名)</p>

<pre><code>データサイエンティスト（名詞）：
統計を行う人材よりもソフトエンジニアに精通し、ソフトエンジニアを行う人間よりも統計に精通した人材
</code></pre>

<p>ウィルの引用は「幻のユニコーン」データサイエンティストを見つけること、そしてデータサイエンティストのような複数分野における修練が必要な仕事においては、最高のものに携わらねばならないといった現実的な問題を反映している。これらの知見はおそらく、データサイエンティストとエンジニアにより近い相互関与をさせるDevopsのようなAgileのpracticeを取り入れ、またそれゆえＰｒｏｄｕｃｔｉｏｎにDeployするといった実質的な経験をさせるウェブジャイアント：GoogleやClouderaといったスタートアップに基礎をおいている。不幸なことにそういうものはレガシーシステムと塩漬けの時代遅れの組織、もしくは大組織の官僚制やマインドセット、詳しい見解の欠如、文化的な隔たりを理由にして常に挑戦的である。</p>

<p>ビッグデータとデータサイエンスの分野の問題にスタートアップやラボが取り組んでいるのだが、誤認をクリアにすることや我々の目標の周辺のシェアされたヴィジョンを培うことを可能にする基本的な共通言語を打ち立てるために、よく使われる用語の理解をチームでシェアすることは我々にとって重要な事柄だ。
それゆえ「幻のユニコーン」データサイエンティストの定義をこえて、また我々のチームのようなデータサイエンティストが働く実際のチームで何が起きているのかを見直すことが必要である。</p>

<h3>異なる知見</h3>

<p>データサイエンティストの多くが自らのことを数学者として考えている、というのもビジネスの問題を数学/統計学の問題の中にまとめようとし、またその時にはデータサイエンスプロジェクトの内部でその問題を処理しようとする。しかしながら、ビッグデータのベンダーや修練の賜物としての超能力とデータサイエンスを同一視するようなエヴァンジェリストによって大量に刷られた誇大広告記事から、よくある誤認が時々立ち上ってくる。開発者の視点はデータのETL、ツールの断片化、技術と言語の複雑さ上のユニークな認識から生じている。</p>

<p>現実はいつも誇大広告とはかなり異なる。そこには実はおそらく、ただ一握りの数学、統計学、AIと機械学習、さまざまなプログラミング言語、そしてさらに幅広い様々なツールと技術、そして当然ビジネスの問題の理解とビジネスの話題の中で複雑なモデルと数学を明確に述べるような超絶の能力をもつ「幻のユニコーン」データサイエンティストがその惑星の上にいるだけなのだ。低い能力者、そして運に見放されたビジネス、私たちはこういった複数の個人たちとこれらのスキルセットをチーム、もしくはデータサイエンス部隊に組み込まなくてはならないのだ。</p>

<h3>データサイエンティストのチームを作る</h3>

<p>雇用に関し、「幻のユニコーン」のデータサイエンティストが本当は使用し得るものではないという考えの周辺に辿り着けば、データサイエンティストのチームを作成するのはかなり簡単になる。リクルートのチームとそのマネージャーはその時、チームの上で求められる個別のスキルに焦点を充てられる、またそれらの個別のスキルに強いプロフィールを雇おうとすることもできる。一度雇えば、マネージャーの仕事はスーパーヒーローになる代わりに、期待されることの設定や協力的なマナーの上で目標を達成することに焦点を当てられるような動的に自治的されたチームに進歩するように仲介し調整するようにチームビルディングをすることへ役割が変わっている。</p>

<h3>データサイエンスチームの役割</h3>

<p>では、データサイエンスのチームはどんな役割があるのだろうか。組織の目標により、そのチームはサービス志向のコンサルタントの仕事、また再利用可能な資産、もしくはデータ製品を打ち立てることに焦点を当てる。</p>

<h3>データサイエンティスト</h3>

<p>この役割は機械学習アルゴリズム、予測モデルの開発、そのモデルの背後にある数学と統計の理論を理解し専門用語なしでの解釈、説明ができる誰かが優先的になるだろう。典型的にこの役割はSQLの能力と例えばRやPythonのような予想データ分析に使用するプログラミング言語を少なくとも一つに秀でている。</p>

<pre><code>資格：統計学、または数学のPhDもしくはMasterの学位、オペレーションリサーチ、CS、もしくは経済学やその他の関係のある分野  
5年以上のビジネス上の重要な影響をきりもりするための分析を利用した追跡可能な証明されうる適切な経験値  
SQLに特化  
統計学的な知識と直観力 理想的にはA/Bテストを利用  
ファイル分散データベースの経験、Hive、もしくはPigのようなクエリ言語の使用経験  
パイソン、ルビー、パールのようなスクリプト言語のプログラミング経験  
情報と結果をクリアに提示するための強力なデータ視覚化能力  
RやSASといった統計分析ツールの熟練度
</code></pre>

<h3>データエンジニア・データソフトウェアエンジニア</h3>

<p>この役割はインフラの理解とアーキテクチャの理解を含んだ分散処理のプログラミングに対して習熟している人物にあてられる。</p>

<p>典型的にはunixスクリプトとSQLをしっていて、Java/Python/Scalaのようなプログラミング言語を一つ以上コードすることが出来、Haddop/MapReduce/Spark のクラスタのような分散型プログラムのフレームワークの環境をよしとする人物だ。
経験の深さと幅を基礎として、この役割は特別な二つの役割に転化する。データプラットフォームの管理者とデータソリューションアーキテクトがその二つだ。</p>

<pre><code>成功に導く技術的な経験（厳格な試金石としてではなく）：Netfilixのデータエンジニア  
mapReduce/Hive/Pig/Sparkその他の経験  
Java, Python,Scalaをコードできるスキル  
ビッグデータとETLの製品サポート経験  
SQLにつよいこと  
定量：機械学習もしくは統計学の基本的な理解を与えるような本、もしくは授業を受けたことがある。  
app/web開発、ソフトウェアエンジニアの原則の理解  
    　
</code></pre>

<h3>データソリューションアーキテクト</h3>

<p>いくつかの分散型の技術に関して深い経験のあるデータエンジニア、またサービス志向の設計コンセプトとウェブアプリケーション（SOAコンセプトとRESTフレームワーク）の理解、また加えて開発者としてのスキルセットをもつデータエンジニア</p>

<h3>データプラットフォーム管理者</h3>

<p>分散型の技術にかんして広範囲な経験をもつ。特に、本番環境をふくむクラスタをマネージした経験とクラウドコンピューティングの設計のよき理解（OpenStackもしくはパブリッククラウドを使用しているのであれば、AWSのようなパブリッククラウド。そしてもしもプライベート/ハイブリッドのクラウドであるならば、リナックスのシスアドの経験）
    　</p>

<h3>フルスタック開発者</h3>

<p>これは選択的な役割だ。ウェブインターフェースでのデータプロダクトの打ち立てに焦点を当てるチームにのみ必要な役割だからだ。フルスタックの開発者は理想的には例としてMongoDB, Express, AngularJS and NodeJSなどの経験の上、MEAN開発者のようなフロントエンドとバックエンド両方に精通した開発者が望ましい。</p>

<h3>デザイナー</h3>

<p>この役割はUXとインターフェースデザインに深く精通したエキスパートが望まれる。主に、データ視覚化、データ製品のターゲットフォームに依存したウェブ、モバイルのアプリケーションのUXデザインとUIコーディングの専門知識を有している。即座にモデルやワイヤーフレームをデザインする事が製品定義の中で求められ、チームワークの流れの中で開発者たち、またビジネス側とも協働して働くことが可能であることが必要になってくる。ときどきこの役割はデザインが得意なフロントエンドのUI開発者によって担われるが、安上りではない。</p>

<pre><code>資格：必須項目として  
5年以上のデザインに関する相互経験（TVのインターフェースなどは望ましい。）  
素晴らしいポートフォリオ  
アドビクリエイティブスーツのエキスパート（動的なものであれば望ましい）  
複数のデザインを即座に清算できる技能  
詳細とマルチタスクへの注意があるという明確な証明  
あなたのプロジェクトをマネージするための修練されたアプローチ
</code></pre>

<h3>プロダクトマネージャー</h3>

<p>この役割はデータ製品を組み立てるためのチームにのみ、またそれがキーワードとして必要になる役割だ。製品のヴィジョンを定義し、ユーザーのストーリーの中にビジネスの問題を落とし込み、ユーザーのストーリーの上でデータ商品をつくるための開発チームをつくり、ビジネスの期待と要求への全体のロードマップと製品のリリースにアライメントを取ることといった内容がこの役割の主な要求になる。適切な技術的な専門知識に沿ったプロダクトマネジメントの経験はITprojectと製品のライフサイクルにおける差異を理由として必須になっている。また、短い期間のニーズと長期のヴィジョンのバランス、顧客の声をプレゼンする能力も必須になる。商品の観点とビジネスへの鋭い視点を持っていない開発者やサイエンティストでこの役割を埋め合わせるのはプロジェクトマネジメントのスキルの不足と金メッキの誘惑のせいで危険なものになる。</p>

<pre><code>最低限の資格：コンピュータサイエンス、技術分野のBA/BSの学位、もしくは相当する経験  
5年以上の企業のソフト、もしくは消費者のインターネットサービスのプロダクトマネージャーとしての経験  

望ましい資格：非常に複雑なビジネスロジックの大きな企業システム、もしくはその他のシステムの経験  
ビッグデータ技術、データウェアハウスのデータマネジメント経験  
協調性があり、クロスファンクショナルな環境における少々のインストラクションと監督によって結果を出すことに慣れているリーダー  
ソリューションを導くデータをデリバリするために、大きな量の情報で働く能力のデモ  
両義的な問題をクリアソリューションに飛躍させることが出来るようなイノベイティブで素晴らしい問題解決能力  
        　　 
</code></pre>

<h3>プロジェクトマネージャー</h3>

<p>この役割は経験に乏しいチームであるときには選択的に求められる。パフォーマンスとして最高のケースは、マネージャーは目標と期待を設定し、アジャイルプラクティスをフォローするような自治するチームに調整する。
    　　</p>

<p>one-offのプロジェクト、もしくは再利用される商品のコンサルティングサービスに焦点をおくデータサイエンスチームにかかわらず、このチームは依然として共通した理解、プロセスに関して、そしてツールと実際の仕事を行うためのプラットフォームとツールに関して最低限の基礎を必要とする。私たちはそういったツールやプラットフォームといったもののためのデータエンジニアリングするための必要項目を次の投稿で見直すこととする。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Study and Memo About Apatche Spark]]></title>
    <link href="http://narusou.github.io/blog/2017/07/18/study-and-memo-about-apatche-spark/"/>
    <updated>2017-07-18T18:36:43+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/18/study-and-memo-about-apatche-spark</id>
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=1DAm9Cr23bg">https://www.youtube.com/watch?v=1DAm9Cr23bg</a></p>

<h3>PyConJPの動画でApatche Sparkの勉強メモ</h3>

<p>・Apatche Sparkは高速分散処理システム。MapReduceと比較すると、中間ファイル作成などがない。<br/>
・多目的なライブラリがあります。 -SparkSQLなど<br/>
・APIが多様です。</p>

<p>DeltaCubeではApathce Sparkで自動化し、クラスタリングしています。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Processing With Spark in R &amp; Python]]></title>
    <link href="http://narusou.github.io/blog/2017/07/18/data-processing-with-spark-in-r-and-python/"/>
    <updated>2017-07-18T14:57:39+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/18/data-processing-with-spark-in-r-and-python</id>
    <content type="html"><![CDATA[<h3>原文</h3>

<p>I recently gave a talk on data processing with Apache Spark using R and Python. tl;dr – the slides and presentation can be accessed below:<br/>
<a href="https://www.brighttalk.com/webcast/9059/172833">https://www.brighttalk.com/webcast/9059/172833</a></p>

<!--more-->


<p>As noted in my previous post, Spark has become the defacto standard for big data applications   and has been adopted quickly by the industry. See Cloudera’s  One Platform initiative blog post by CEO Mike Olson for their commitment to Spark.　In data science R had seen rapid adoption, not only because it was open source and free   compared to costly SAS, but also the huge number of statistical and graphical packages provided　by R for data science. The most popular ones of course are the ones from Hadley Wickham (dplyr,　ggplot2, reshape2, tidyr and more). On the other hand, Python had seen rapid adoption among　developers and engineers due to its being useful to script big data tasks along with data　analysis with the help of packages like pandas, scikit-learn, NumPy, SciPy, matplotlib etc.　and also the popular iPython &amp; later Jupyter notebooks.　</p>

<p>There are numerous posts strewn on the net picking fights between R and Python. However it is quite usual for any big data and data science shop to have developers and data scientists who use either or both these tools. Spark makes it easy for both communities to leverage the power of Hadoop and distributed processing systems with its own APIs like DataFrames which can be used in a polyglot fashion. Therefore it is essential for any data enthusiast to learn about how data processing in Spark can be done using R or Python.</p>

<h3>翻訳</h3>

<p>最近、RとPythonを使用したApatche Sparkによるデータプロセスに関して話をした。下記のURLからスライドとプレゼンテーションにアクセス可能。</p>

<p><a href="https://www.brighttalk.com/webcast/9059/172833">https://www.brighttalk.com/webcast/9059/172833</a></p>

<p>過去の投稿にあるように、Sparkはビックデータアプリケーションの事実上のスタンダードになり、また急速にBI業界に取り入れられた経緯がある。CEO Mike Olsonによる彼らのSparkへのコミットメントに関するブログ投稿を参照。
データサイエンスに関して、RはコストがかかるSASに比べてフリーでオープンソースであるという理由のみならず、データサイエンスのために用意された統計的でグラフィカルな数多くのパッケージによって急速に普及してきた。もっともよく知られている事例としてはHadley Wickham(dplyr, ggplot2, reshape2, tidyr and more)による。一方、Pythonは開発者とエンジニアの間で、pandasやscikit-learn、NumPy, SciPy,matplotlibとかのようなパッケージによってデータアナリシスにおけるビッグデータのタスクを記述するのに便利であるため急速に採用されてきた。そして、iPython, 後期のJupiter notebooksによるものがよく知られている。</p>

<p>非常に多くのRとPythonに関する両社の批判や議事が散らばった投稿として見受けられる。しかし、これらを同時に両方、もしくは二種類を使用するデータサイエンティストと開発者達を保有するのはデータサイエンスやビッグデータを生業にするものにとってはかなり普通のことと思われる。SparkはHadoopの力と、多言語仕様で使用することが出来るデータフレームのようなSparkのAPIと、それを使用した分散処理プロセスシステムを、この両者のコミュニティが利用することを簡単にする。こういった背景から、RとPyhonによるSparkが行うデータプロセスがどのようなものなのかを学ぶことはデータマニアにとっては必要不可欠なことだと思われる。</p>

<h3>出典</h3>

<p><a href="https://biguru.wordpress.com/2015/11/18/data-processing-with-spark-in-r-python/">https://biguru.wordpress.com/2015/11/18/data-processing-with-spark-in-r-python/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Translation1]]></title>
    <link href="http://narusou.github.io/blog/2017/07/18/translation1/"/>
    <updated>2017-07-18T14:56:11+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/18/translation1</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Test]]></title>
    <link href="http://narusou.github.io/blog/2017/07/15/test/"/>
    <updated>2017-07-15T22:20:27+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/15/test</id>
    <content type="html"><![CDATA[<p>This site is created for my practice to develop homepages and traslate articles about Business Intelligence.</p>

<!--more-->


<p><img src="http://narusou.github.io/images/test.png" alt="test" /></p>

<h2>環境</h2>

<p>PC:HPchromebook<br/>
OS:Google Chrome and UBUNTU 14.04 LTS</p>

<h2>OctPressをGITHUBにdeployするためにしたこと</h2>

<pre><code>1,githubにブランチ作成
2,rbenvのインストール
3,rubｙのインストール
4,githubとlocalの同期
5,rakeで記事の作成
6,generate, deploy
</code></pre>

<h2>躓いた点</h2>

<p> ・rubyとrbenv versionの整理<br/>
 ・git pull</p>

<h2>使用したコード 順不同</h2>

<pre><code>$ rake deploy
$ rake generate
$ ruby -v
$ ruby --version
$ rbenv versions
$ rbenv version
$ source ~/.bash_profile
$ echo $PATH
$ which ruby
$ which rbenv
$ rbenv init
$ rbenv global 2.4.1
$ bundle exec 
$ git pull
$ git push
$ git commit
$ sudo apt-get install ruby
$ sudo apt-get install git
$ git clone git://github.com/sstephenson/rbenv.git ~/.rbenv
$ git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build
$ echo 'export PATH="$HOME/.rbenv/bin:$PATH"' &gt;&gt; ~/.profile
$ echo 'eval "$(rbenv init -)"' &gt;&gt; ~/.profile
$ exec $SHELL -l
$ rbenv install -l
$ rbenv install 2.4.1
$ rbenv exec gem install bundler
$ rbenv rehash
$ git clone git://github.com/imathis/octopress.git octopress
$ cd octopress
$ bundle install --path vendor/bundle
$ git clone git://github.com/lucaslew/whitespace.git .themes/whitespace
$ rake install['whitespace'] # for zsh, use: rake install\['whitespace'\]
$ rake new_post['test']
$ rake preview
$ rake setup_github_pages
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First_test]]></title>
    <link href="http://narusou.github.io/blog/2017/07/15/first-test/"/>
    <updated>2017-07-15T21:36:23+09:00</updated>
    <id>http://narusou.github.io/blog/2017/07/15/first-test</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
</feed>
